{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Building a Modular Forecasting Framework: Fine-Tuning and Predicting Offshore Wind Generation\n",
    "\n",
    "---\n",
    "\n",
    "Forecasting energy-related quantities is a critical component of managing renewable energy systems, especially in the context of increasing reliance on wind and solar power. In this series of articles, I detail the development of a scalable MLOps forecasting framework designed to predict energy-related variables for Germany several days ahead. This framework combines open data sources, advanced machine learning models, and a modular, task-based design to ensure reusability and versatility.\n",
    "In Part 1, I introduced the pipeline for data collection and preprocessing, showcasing how API integrations and preprocessing steps standardize historical and forecasted data for modeling. In Part 2, I implemented baseline forecasting models, including ElasticNet, Prophet, and XGBoost, and developed an ensemble model to improve accuracy. These models were modularized for flexibility.\n",
    "This article focuses on the next stage: designing an optimized, modular framework for fine-tuning, training, and forecasting. Using offshore wind generation as the target variable, I demonstrate how careful feature engineering, automatic hyperparameter optimization with Optuna, and a task-based approach enable accurate multi-step predictions. By prioritizing modularity and computation efficiency, the framework ensures scalability for forecasting additional energy-related quantities in the future.\n",
    "\n",
    "---"
   ],
   "id": "242a6401b1643771"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import os, gc\n",
    "import copy\n",
    "import holidays\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess, Fourier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_dir = '../../database/'\n",
    "target = 'wind_offshore'\n",
    "# df = pd.read_parquet(data_dir + 'latest.parquet')\n",
    "df_smard = pd.read_parquet(data_dir + 'smard/' + 'history.parquet')\n",
    "df_om = pd.read_parquet(data_dir + 'openmeteo/' + 'history.parquet')\n",
    "df_om_f = pd.read_parquet(data_dir + 'openmeteo/' + 'forecast.parquet')\n",
    "df_es = pd.read_parquet(data_dir + 'epexspot/' + 'history.parquet')\n",
    "\n",
    "print(f\"SMARD data shapes hist={df_smard.shape} start={df_smard.index[0]} end={df_smard.index[-1]}\")\n",
    "print(f\"Openmeteo data shapes hist={df_om.shape} start={df_om.index[0]} end={df_om.index[-1]}\")\n",
    "print(f\"Openmeteo data shapes forecast={df_om_f.shape} start={df_om_f.index[0]} end={df_om_f.index[-1]}\")\n",
    "print(f\"EPEXSPOT data shapes hist={df_es.shape} start={df_es.index[0]} end={df_es.index[-1]}\")\n",
    "\n",
    "print(f\"Target={target} Nans={df_smard[target].isna().sum().sum()}\")\n",
    "# set how to split the dataset\n",
    "cutoff = df_om_f.index[0]\n",
    "if cutoff == cutoff.normalize():\n",
    "    print(f\"The cutoff timestamp corresponds to the beginning of the day {cutoff.normalize()}\")\n",
    "print(f\"Dataset is split into ({len(df_om[:cutoff])}) before and \"\n",
    "      f\"({len(df_om_f[cutoff:])}) ({int(len(df_om_f[cutoff:])/24)} days) after {cutoff}.\")\n",
    "print(df_smard.columns.to_list())"
   ],
   "id": "ab5cf83a2e895dd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Feature Engineering for Offshore Wind Generation\n",
    "Accurate forecasting of offshore wind energy generation requires translating raw weather data into features that are more directly related to the physical processes driving wind power. In this project, I developed a preprocessing function to extract and compute critical derived features from weather data for offshore wind farms. The process was guided by literature on renewable energy modeling and tailored specifically for Nordsee Windparks EnBW Hohe See, a prominent offshore wind farm in Germany."
   ],
   "id": "aacb21071d25ad96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create combined dataframe\n",
    "cutoff = df_om_f.index[0]\n",
    "df_om = df_om.combine_first(df_om_f)\n",
    "if df_om.isna().any().any():\n",
    "    print(\"ERROR! Nans in the dataframe\")\n",
    "\n",
    "def preprocess_openmeteo_for_offshore_wind(df, location_suffix=\"_hsee\")->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses weather data for forecasting offshore wind energy generation.\n",
    "    Focuses on critical physical features and includes turbulence_intensity, wind_ramp, and wind_shear.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Filter for the offshore wind farm location only\n",
    "    cols_to_keep = [c for c in df.columns if c.endswith(location_suffix)]\n",
    "    df = df[cols_to_keep].copy()\n",
    "\n",
    "    # 2. Define key variable columns\n",
    "    wind_speed_10m_col = f\"wind_speed_10m{location_suffix}\"\n",
    "    wind_speed_100m_col = f\"wind_speed_100m{location_suffix}\"\n",
    "    wind_dir_100m_col = f\"wind_direction_100m{location_suffix}\"\n",
    "    temp_col = f\"temperature_2m{location_suffix}\"\n",
    "    press_col = f\"surface_pressure{location_suffix}\"\n",
    "\n",
    "    # 3. Compute Air Density (ρ)\n",
    "    if temp_col in df.columns and press_col in df.columns:\n",
    "        temp_K = df[temp_col] + 273.15\n",
    "        R_specific = 287.05  # J/(kg·K) for dry air\n",
    "        # convert pressure from hPa to Pa\n",
    "        df[\"air_density\"] = np.array( (df[press_col] * 100.) / (R_specific * temp_K) )\n",
    "\n",
    "    # 4. Compute Wind Power Density (if wind_speed_100m and air_density are available)\n",
    "    if wind_speed_100m_col in df.columns and \"air_density\" in df.columns:\n",
    "        df[\"wind_power_density\"] = np.array( 0.5 * df[\"air_density\"] * (df[wind_speed_100m_col] ** 3) )\n",
    "\n",
    "    # 5. Encode Wind Direction (Cyclic)\n",
    "    if wind_dir_100m_col in df.columns:\n",
    "        df[\"wind_dir_sin\"] = np.sin(np.deg2rad(df[wind_dir_100m_col]))\n",
    "        df[\"wind_dir_cos\"] = np.cos(np.deg2rad(df[wind_dir_100m_col]))\n",
    "        df.drop(columns=[wind_dir_100m_col], inplace=True)\n",
    "\n",
    "    # 6. Wind Shear (Requires both 10m and 100m wind speeds)\n",
    "    if wind_speed_10m_col in df.columns and wind_speed_100m_col in df.columns:\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df[\"wind_shear\"] = np.log(df[wind_speed_100m_col] / df[wind_speed_10m_col]) / np.log(100/10)\n",
    "        # Replace infinities or NaNs if they occur\n",
    "        df[\"wind_shear\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 7. Turbulence Intensity (using a short rolling window on 100m wind speed)\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        rolling_std = df[wind_speed_100m_col].rolling(window=3, min_periods=1).std()\n",
    "        rolling_mean = df[wind_speed_100m_col].rolling(window=3, min_periods=1).mean()\n",
    "        df[\"turbulence_intensity\"] = np.array( rolling_std / rolling_mean )\n",
    "\n",
    "    # 8. Wind Ramp (difference in 100m wind speed over 1 timestep)\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        df[\"wind_ramp\"] = df[wind_speed_100m_col].diff(1)\n",
    "\n",
    "    # 9. Lag Features for Wind Speed at 100m\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        for lag in [1, 6, 12, 24]:\n",
    "            df[f\"wind_speed_lag_{lag}\"] = df[wind_speed_100m_col].shift(lag)\n",
    "\n",
    "    # 11. Drop Irrelevant Columns\n",
    "    # Decide which columns to drop. For model simplicity, consider dropping raw weather inputs\n",
    "    # that have been transformed into more physical parameters.\n",
    "    # However, keep wind speeds if you think they add value.\n",
    "    # For now, we keep the wind speeds since other derived features depend on them.\n",
    "    drop_vars = [\n",
    "        temp_col, press_col, \"air_density\",\n",
    "        f\"precipitation{location_suffix}\",\n",
    "        f\"cloud_cover{location_suffix}\",\n",
    "        f\"shortwave_radiation{location_suffix}\",\n",
    "        f\"relative_humidity_2m{location_suffix}\",\n",
    "        f\"wind_direction_10m{location_suffix}\",\n",
    "        f\"wind_gusts_10m{location_suffix}\"\n",
    "    ]\n",
    "    drop_cols = [c for c in drop_vars if c in df.columns]\n",
    "    df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Handle missing values introduced by lagging and other computations\n",
    "    # df.dropna(inplace=True)\n",
    "\n",
    "    return df\n",
    "# Extract data for the offshore wind farm location and create new features\n",
    "df_om_prep = preprocess_openmeteo_for_offshore_wind(df=df_om, location_suffix=\"_hsee\")\n",
    "df_om_prep.dropna(inplace=True) # drop nans formed when lagged features were added"
   ],
   "id": "d7571ef12e0afce2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_weather_smard(df_om_prep_:pd.DataFrame, df_smard_:pd.DataFrame, target:str, ylabel:str):\n",
    "    fig, axes = plt.subplots(ncols=1, nrows=6, figsize=(8, 8), sharex='all')\n",
    "    # Plot data\n",
    "    axes[0].plot(df_smard_.index, df_smard_[target], label=target, color='black', lw=0.6)\n",
    "    axes[0].set_ylabel(ylabel)\n",
    "\n",
    "    axes[1].plot(df_om_prep_.index, df_om_prep_[\"wind_speed_100m_hsee\"], label=\"Wind Speed 100m\", color='black', lw=0.6)\n",
    "    axes[1].set_ylabel(\"Wind Speed 100m\")\n",
    "\n",
    "    axes[2].plot(df_om_prep_.index, df_om_prep_[\"wind_shear\"], label=\"Wind Shear\", color='black', lw=0.6)\n",
    "    axes[2].set_ylabel(\"Wind Shear\")\n",
    "\n",
    "    axes[3].plot(df_om_prep_.index, df_om_prep_[\"turbulence_intensity\"], label=\"Turbulence Intensity\", color='black', lw=0.6)\n",
    "    axes[3].set_ylabel(\"Turbulence Intensity\")\n",
    "\n",
    "    axes[4].plot(df_om_prep_.index, df_om_prep_[\"wind_ramp\"], label=\"Wind Ramp\", color='black', lw=0.6)\n",
    "    axes[4].set_ylabel(\"Wind Ramp\")\n",
    "\n",
    "    axes[5].plot(df_om_prep_.index, df_om_prep_[\"wind_power_density\"], label=\"Wind Power Density\", color='black', lw=0.6)\n",
    "    axes[5].set_ylabel(\"Wind Power Density\")\n",
    "\n",
    "    # Configure transparent axes and add gray gridlines\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.grid(True, linestyle='-', alpha=0.4)\n",
    "        ax.tick_params(axis='x', direction='in', bottom=True)\n",
    "        ax.tick_params(axis='y', which='both', direction='in', left=True, right=True)\n",
    "        # Set border lines transparent by setting the edge color and alpha\n",
    "        ax.spines['top'].set_edgecolor((1, 1, 1, 0))  # Transparent top border\n",
    "        ax.spines['right'].set_edgecolor((1, 1, 1, 0))  # Transparent right border\n",
    "        ax.spines['left'].set_edgecolor((1, 1, 1, 0))  # Transparent left border\n",
    "        ax.spines['bottom'].set_edgecolor((1, 1, 1, 0))  # Transparent bottom border\n",
    "\n",
    "        # Make x and y ticks transparent\n",
    "        ax.tick_params(axis='x', color=(1, 1, 1, 0))  # Transparent x ticks\n",
    "        ax.tick_params(axis='y', color=(1, 1, 1, 0))  # Transparent y ticks\n",
    "\n",
    "        if i == len(axes) - 1:\n",
    "            ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "            # ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format as \"Dec 15\"\n",
    "        fig.autofmt_xdate(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('./engineered_features.png',dpi=600)\n",
    "    plt.show()\n",
    "visualize_weather_smard(df_om_prep[:cutoff].tail(30*24), df_smard[:cutoff].tail(30*24), target=target, ylabel='Offs. Wind Power')"
   ],
   "id": "b983c5d5e35db7ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_correlations(df_om_prep_: pd.DataFrame, df_smard_: pd.DataFrame, target: str, label:str):\n",
    "    # Ensure the target column exists in df_smard_\n",
    "    if target not in df_smard_.columns:\n",
    "        raise ValueError(f\"Target column '{target}' not found in the provided df_smard_ DataFrame.\")\n",
    "\n",
    "    # Extract the target series\n",
    "    target_series = df_smard_[target]\n",
    "\n",
    "    # Compute correlations with predictors in df_om_prep_\n",
    "    correlations = df_om_prep_.apply(lambda col: col.corr(target_series))\n",
    "\n",
    "    # Drop NaN values (e.g., if correlation couldn't be computed)\n",
    "    correlations = correlations.dropna()\n",
    "\n",
    "    # Format feature names for display\n",
    "    formatted_feature_names = [\n",
    "        col.replace('_', ' ').title() for col in correlations.index\n",
    "    ]\n",
    "\n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Bar plot for correlations\n",
    "    bars = ax.barh(formatted_feature_names, correlations.values, color='black', alpha=0.8)\n",
    "\n",
    "    # Add gridlines\n",
    "    ax.grid(True, linestyle='-', alpha=0.4, axis='x')\n",
    "\n",
    "    # Set transparent borders\n",
    "    ax.spines['top'].set_edgecolor((1, 1, 1, 0))\n",
    "    ax.spines['right'].set_edgecolor((1, 1, 1, 0))\n",
    "    ax.spines['left'].set_edgecolor((1, 1, 1, 0))\n",
    "    ax.spines['bottom'].set_edgecolor((1, 1, 1, 0))\n",
    "\n",
    "    # Set x and y ticks transparent\n",
    "    ax.tick_params(axis='x', color=(1, 1, 1, 0))\n",
    "    ax.tick_params(axis='y', color=(1, 1, 1, 0))\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(\"Correlation\", color='black')\n",
    "    ax.set_ylabel(\"Features\", color='black')\n",
    "    ax.set_title(f\"Correlation with {label}\", color='black')\n",
    "\n",
    "    # Add values on bars\n",
    "    for bar in bars:\n",
    "        ax.text(\n",
    "            bar.get_width(),\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f'{bar.get_width():.2f}',\n",
    "            va='center',\n",
    "            ha='left',\n",
    "            color='gray',\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./feature_target_correlations.png',dpi=600)\n",
    "    plt.show()\n",
    "plot_correlations(df_om_prep[:cutoff], df_smard[:cutoff], target=target, label='Offshore Wind Power')\n"
   ],
   "id": "ce949e10e4708718",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task-Based Modular Pipeline",
   "id": "98d4f709f7152ce2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def handle_nans_with_interpolation(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks each column of the DataFrame for NaNs. If a column has more than 3 consecutive NaNs,\n",
    "    it raises a ValueError. Otherwise, fills the NaNs using bi-directional interpolation.\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    def check_consecutive_nans(series: pd.Series):\n",
    "        # Identify consecutive NaNs by grouping non-NaN segments and counting consecutive NaNs\n",
    "        consecutive_nans = (series.isna().astype(int)\n",
    "                            .groupby((~series.isna()).cumsum())\n",
    "                            .cumsum())\n",
    "        if consecutive_nans.max() > 3:\n",
    "            raise ValueError(f\"Column '{series.name}' in {name} contains more than 3 consecutive NaNs.\")\n",
    "\n",
    "    # Check all columns for consecutive NaNs first\n",
    "    for col in df_copy.columns:\n",
    "        check_consecutive_nans(df_copy[col])\n",
    "\n",
    "    # Interpolate all columns at once after confirming they're valid\n",
    "    df_copy = df_copy.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "\n",
    "    return df_copy\n",
    "def fix_broken_periodicity_with_interpolation(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fixes broken hourly periodicity by adding missing timestamps if fewer than 3 consecutive are missing.\n",
    "    Raises an error if more than 3 consecutive timestamps are missing.\n",
    "    Missing values are filled using time-based interpolation.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(f\"The DataFrame {name} must have a datetime index.\")\n",
    "\n",
    "    expected_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "    missing_timestamps = expected_index.difference(df.index)\n",
    "\n",
    "    if missing_timestamps.empty:\n",
    "        print(f\"The DataFrame {name} is already hourly with no missing segments.\")\n",
    "        return df\n",
    "\n",
    "    # Convert to a Series to check consecutive missing timestamps\n",
    "    missing_series = pd.Series(missing_timestamps)\n",
    "    groups = (missing_series.diff() != pd.Timedelta(hours=1)).cumsum()\n",
    "\n",
    "    # Check if any group has more than 3 missing points\n",
    "    group_counts = groups.value_counts()\n",
    "    if (group_counts > 3).any():\n",
    "        bad_group = group_counts[group_counts > 3].index[0]\n",
    "        raise ValueError(f\"More than 3 consecutive missing timestamps detected: \"\n",
    "                         f\"{missing_series[groups == bad_group].values} in {name}\")\n",
    "\n",
    "    # Reindex and interpolate\n",
    "    fixed_df = df.reindex(expected_index)\n",
    "    fixed_df = fixed_df.interpolate(method='time')\n",
    "\n",
    "    print(f\"Added and interpolated {len(missing_timestamps)} missing timestamps in {name}.\")\n",
    "\n",
    "    return fixed_df\n",
    "def validate_dataframe(df: pd.DataFrame, name: str = '') -> pd.DataFrame:\n",
    "    \"\"\"Check for NaNs, missing values, and periodicity in a time-series DataFrame.\"\"\"\n",
    "\n",
    "    # Check for NaNs\n",
    "    if df.isnull().any().any():\n",
    "        print(f\"ERROR! {name} DataFrame contains NaN values.\")\n",
    "        df = handle_nans_with_interpolation(df, name)\n",
    "\n",
    "    # Check if index is sorted in ascending order\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        print(f\"ERROR! {name} The index is not in ascending order.\")\n",
    "        raise ValueError(\"Data is not in ascending order.\")\n",
    "\n",
    "    # Check for hourly frequency with no missing segments\n",
    "    full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "    if not full_range.equals(df.index):\n",
    "        print(f\"ERROR! {name} The data is not hourly or has missing segments.\")\n",
    "        df = fix_broken_periodicity_with_interpolation(df, name)\n",
    "\n",
    "    return df\n",
    "\n",
    "horizon = 7 * 24\n",
    "# merger with SMRD target column\n",
    "target = 'wind_offshore'\n",
    "df_om_prep.dropna(inplace=True, how='any')\n",
    "df_hist = pd.merge(\n",
    "    df_om_prep[:cutoff-timedelta(hours=1)],\n",
    "    df_smard[:cutoff-timedelta(hours=1)][target],\n",
    "    left_index=True, right_index=True, how=\"inner\"\n",
    ")\n",
    "df_forecast = df_om_prep[cutoff : cutoff+timedelta(hours=horizon - 1)]\n",
    "df_hist = validate_dataframe(df_hist, 'df_hist')\n",
    "df_forecast = validate_dataframe(df_forecast, 'df_forecast')\n",
    "df_hist = df_hist[df_hist.index[-1]-pd.Timedelta(hours = 100 * horizon - 1):]\n",
    "print(f\"Features {len(df_hist.columns)-1} hist.shape={df_hist.shape} ({int(len(df_hist)/horizon)}) forecast.shape={df_forecast.shape}\")\n",
    "print(f\"Hist: from {df_hist.index[0]} to {df_hist.index[-1]} ({len(df_hist)/horizon})\")\n",
    "print(f\"Fore: from {df_forecast.index[0]} to {df_forecast.index[-1]} ({len(df_forecast)/horizon})\")\n",
    "\n",
    "# df_hist.to_parquet(f\"{output_dir}history.parquet\")\n",
    "# df_forecast.to_parquet(f\"{output_dir}forecast.parquet\")"
   ],
   "id": "d78d920085daf845",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from forecasting_modules import ForecastingTaskSingleTarget\n",
    "def main():\n",
    "    tasks = [\n",
    "        {\n",
    "            \"target\": \"wind_offshore\",\n",
    "            \"label\": \"Wind off-shore [MW]\",\n",
    "            \"features\":[],\n",
    "            \"task_fine_tuning\":[\n",
    "                # {'model':'Prophet',\n",
    "                #  'dataset_pars':{\n",
    "                #      'forecast_horizon':None,\n",
    "                #      'target_scaler':'StandardScaler',\n",
    "                #      'feature_scaler':'StandardScaler',\n",
    "                #      'feature_pca_pars' : None,\n",
    "                #      'limit_pca_to_features':None,#'weather',\n",
    "                #      'fourier_features' : {'period':24, 'order':3},\n",
    "                #      'add_cyclical_time_features':True,\n",
    "                #      'lags_target':None,'log_target':True,\n",
    "                #      'copy_input':True\n",
    "                #  },\n",
    "                # 'finetuning_pars':{'n_trials':120,'optim_metric':'rmse','cv_folds':3}},\n",
    "\n",
    "                # {'model':'XGBoost',\n",
    "                #  'dataset_pars':{\n",
    "                #      'forecast_horizon':None,\n",
    "                #      'target_scaler':'StandardScaler',\n",
    "                #      'feature_scaler':'StandardScaler',\n",
    "                #      'limit_pca_to_features':None,#'weather',\n",
    "                #      'feature_pca_pars':None,#{'n_components':0.95},\n",
    "                #      'fourier_features': {},\n",
    "                #      'add_cyclical_time_features':True,\n",
    "                #      'lags_target':24,'log_target':True,\n",
    "                #      'copy_input':True\n",
    "                #  },\n",
    "                #  'finetuning_pars':{'n_trials':5,'optim_metric':'rmse','cv_folds':3}},\n",
    "                #\n",
    "                # {'model':'ElasticNet',\n",
    "                #  'dataset_pars':{\n",
    "                #      'forecast_horizon':None,\n",
    "                #      'target_scaler':'StandardScaler',\n",
    "                #      'feature_scaler':'StandardScaler',\n",
    "                #      'limit_pca_to_features':None,#'weather',\n",
    "                #      'feature_pca_pars':None,#{'n_components':0.95},\n",
    "                #      'fourier_features': {},\n",
    "                #      'add_cyclical_time_features':True,\n",
    "                #      'lags_target':24,'log_target':True,\n",
    "                #      'copy_input':True\n",
    "                #  },\n",
    "                #  'finetuning_pars':{'n_trials':5,'optim_metric':'rmse','cv_folds':3}},\n",
    "                #\n",
    "                # {'model':'ensemble[XGBoost](XGBoost,ElasticNet)',\n",
    "                #  'dataset_pars': {\n",
    "                #      'forecast_horizon':None,\n",
    "                #      'target_scaler':'StandardScaler',\n",
    "                #      'feature_scaler':'StandardScaler',\n",
    "                #      'limit_pca_to_features':None,#'weather',\n",
    "                #      'feature_pca_pars':None,#{'n_components':0.95},\n",
    "                #      'add_cyclical_time_features':False,\n",
    "                #      'fourier_features': {},\n",
    "                #      'ensemble_features': 'cyclic_time',\n",
    "                #      'log_target':True,\n",
    "                #      'lags_target': None,\n",
    "                #      'copy_input':True\n",
    "                #\n",
    "                #  },\n",
    "                #  'finetuning_pars':{'n_trials':5,\n",
    "                #                     'optim_metric':'rmse',\n",
    "                #                     'cv_folds':3,\n",
    "                #                     'cv_folds_base':35,\n",
    "                #                     'use_base_models_pred_intervals':False}}\n",
    "            ],\n",
    "            \"task_training\":[\n",
    "                # {'model':'Prophet', 'pars':{'cv_folds':5}},\n",
    "                {'model':'XGBoost', 'pars':{'cv_folds':5}},\n",
    "                {'model':'ElasticNet', 'pars':{'cv_folds':5}},\n",
    "                {'model':'ensemble[XGBoost](XGBoost,ElasticNet)','pars':{'cv_folds':5}},\n",
    "                {'model':'ensemble[ElasticNet](XGBoost,ElasticNet)','pars':{'cv_folds':5}}\n",
    "            ],\n",
    "            \"task_forecasting\":[\n",
    "                # {'model':'Prophet'},\n",
    "                {'model':'XGBoost', 'past_folds':5},\n",
    "                {'model':'ElasticNet', 'past_folds':5},\n",
    "                {'model':'ensemble[XGBoost](XGBoost,ElasticNet)','past_folds':5},\n",
    "                {'model':'ensemble[ElasticNet](XGBoost,ElasticNet)','past_folds':5}\n",
    "            ],\n",
    "            \"task_plot\":[\n",
    "                # {'model':'Prophet', 'n':2,\n",
    "                #  'name':\"Prophet\",'lw':0.7,'color':\"red\",'ci_alpha':0.0},\n",
    "                # {'model':'XGBoost','n':2,\n",
    "                #  'name':'XGBoost','lw':0.7,'color':\"green\",'ci_alpha':0.0,\n",
    "                #  'train_forecast':'train'},\n",
    "                # {'model':'ElasticNet','n':2,\n",
    "                #  'name':'ElasticNet','lw':0.7,'color':\"blue\",'ci_alpha':0.0,\n",
    "                #  'train_forecast':'train'},\n",
    "                # {'model':'ensemble[XGBoost](XGBoost,ElasticNet)','n':2,\n",
    "                #  'name':'Ensemble','lw':1.0,'color':\"purple\",'ci_alpha':0.2,\n",
    "                #  'train_forecast':'train'},\n",
    "                # {'model':'ensemble[ElasticNet](XGBoost,ElasticNet)','n':2,\n",
    "                #  'name':'Ensemble','lw':1.0,'color':\"magenta\",'ci_alpha':0.2,\n",
    "                #  'train_forecast':'train'},\n",
    "            ],\n",
    "            \"task_summarize\":[\n",
    "                # {'model':'Prophet', 'pars':{'cv_folds':5}},\n",
    "                {'model':'XGBoost', 'summary_metric':'rmse'},\n",
    "                {'model':'ElasticNet', 'summary_metric':'rmse'},\n",
    "                {'model':'ensemble[XGBoost](XGBoost,ElasticNet)', 'summary_metric':'rmse'},\n",
    "                {'model':'ensemble[ElasticNet](XGBoost,ElasticNet)', 'summary_metric':'rmse'},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    outdir = '../../forecasting_modules/output/'\n",
    "    # datapath = '../tmp_database/' # expected to fild history.parquet and forecast.parquet\n",
    "\n",
    "    if not os.path.isdir(outdir):\n",
    "        os.mkdir(outdir)\n",
    "\n",
    "    for task in tasks:\n",
    "        target = task['target']\n",
    "        processor = ForecastingTaskSingleTarget(\n",
    "            df_history=df_hist,df_forecast=df_forecast,task=task,outdir=outdir,verbose=True\n",
    "        )\n",
    "\n",
    "        # process task to fine-tune the forecasting model\n",
    "        if task['task_fine_tuning']:\n",
    "            for ft_task in task['task_fine_tuning']:\n",
    "                if ft_task['model'].__contains__('ensemble'):\n",
    "                    processor.process_finetuning_task_ensemble(ft_task)\n",
    "                else:\n",
    "                    processor.process_finetuning_task_base(ft_task)\n",
    "\n",
    "        # train forecasting model on full dataset assuming hyperparameters are in finetuning dir\n",
    "        if task['task_training']:\n",
    "            for t_task in task['task_training']:\n",
    "                if t_task['model'].__contains__('ensemble'):\n",
    "                    processor.process_training_task_ensemble(t_task)\n",
    "                else:\n",
    "                    processor.process_training_task_base(t_task)\n",
    "\n",
    "        # forecast with trained model\n",
    "        if task['task_forecasting']:\n",
    "            for t_task in task['task_forecasting']:\n",
    "                if t_task['model'].__contains__('ensemble'):\n",
    "                    processor.process_forecasting_task_ensemble(t_task)\n",
    "                else:\n",
    "                    processor.process_forecasting_task_base(t_task)\n",
    "\n",
    "        if task['task_plot']:\n",
    "            processor.process_task_plot_predict_forecast(task)\n",
    "\n",
    "        if task['task_summarize']:\n",
    "            processor.process_task_determine_the_best_model(task, outdir=outdir+target+'/')\n",
    "main()"
   ],
   "id": "21b578273d78863e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Automatic Fine-Tuning with Optuna",
   "id": "4e254f84c33d51c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from forecasting_modules.interface import TaskPaths\n",
    "def load_and_plot_horizontal_histograms(outdir: str):\n",
    "    \"\"\"\n",
    "    Load the complete study results from a CSV file and create horizontal histograms for each parameter.\n",
    "\n",
    "    Args:\n",
    "        outdir (str): Path to the directory containing the study results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    complete_results_path = os.path.join(outdir, '_complete_study_results.csv')\n",
    "    if not os.path.isfile(complete_results_path):\n",
    "        raise FileNotFoundError(f\"Complete study results file not found: {complete_results_path}\")\n",
    "\n",
    "    df = pd.read_csv(complete_results_path)\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    if 'state' in df.columns:\n",
    "        df = df[df['state'] == 'COMPLETE']  # Only consider completed trials\n",
    "        df.drop(columns=['state'], inplace=True)\n",
    "\n",
    "    # Determine the best parameter values\n",
    "    best_params = df.loc[df['value'].idxmin()] if 'value' in df.columns else None\n",
    "\n",
    "    df.drop(columns=['number', 'value'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Plot horizontal histograms for each parameter\n",
    "    num_params = len(df.columns)\n",
    "    fig, axes = plt.subplots(1, num_params, figsize=(8, 3), sharey=False)\n",
    "\n",
    "    if num_params == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable if there's only one parameter\n",
    "\n",
    "    for ax, column in zip(axes, df.columns):\n",
    "        data = df[column].dropna()\n",
    "        # Determine the number of bins based on the number of samples, limited to a maximum\n",
    "        num_bins = min(50, max(5, int(len(data) ** 0.5)))\n",
    "\n",
    "        counts, bins, patches = ax.hist(\n",
    "            data, bins=num_bins, orientation='horizontal', color='gray', edgecolor='black', alpha=0.6\n",
    "        )\n",
    "        ax.set_ylabel(column, fontsize=9)  # Move the column name to the y-axis label\n",
    "\n",
    "        # Check for insufficient exploration (low variance or concentration near bounds)\n",
    "        is_low_variance = data.std() < (data.max() - data.min()) * 0.1\n",
    "        is_edge_concentrated = (data < data.min() + (data.max() - data.min()) * 0.1).mean() > 0.5 or \\\n",
    "                               (data > data.max() - (data.max() - data.min()) * 0.1).mean() > 0.5\n",
    "        if is_low_variance or is_edge_concentrated:\n",
    "            for patch in patches:\n",
    "                patch.set_hatch('////')  # Add hashing to the histogram boxes\n",
    "\n",
    "        # Find the largest bin and annotate it\n",
    "        max_count_idx = counts.argmax()\n",
    "        max_count = counts[max_count_idx]\n",
    "        bin_center = (bins[max_count_idx] + bins[max_count_idx + 1]) / 2\n",
    "        # ax.text(max_count, bin_center, str(int(max_count)), ha='left', va='center', fontsize=8, color='black')\n",
    "\n",
    "        # Add a horizontal line at the best parameter value if available\n",
    "        if best_params is not None and column in best_params:\n",
    "            best_value = best_params[column]\n",
    "            ax.axhline(y=best_value, color='black', linestyle='-', linewidth=2)#, label='Best Value')\n",
    "            # ax.legend(fontsize=8)\n",
    "\n",
    "\n",
    "        # Remove x-axis ticks and labels\n",
    "        ax.set_xticks([])\n",
    "        ax.grid(color='white')\n",
    "\n",
    "        # Remove axis lines\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    fig.suptitle(f\"Optuna Study Results. Best trial {int(best_params['number'])}/{len(df)} with RMSE={best_params['value']:.1f}\", fontsize=12, y=0.92)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./optuna_study_result.png', dpi=600)\n",
    "    plt.show()\n",
    "path = TaskPaths(target=target, model_label='XGBoost',working_dir='../../forecasting_modules/output/',verbose=True)\n",
    "load_and_plot_horizontal_histograms(outdir=path.to_finetuned())"
   ],
   "id": "646ea724c244bdb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from forecasting_modules import compute_timeseries_split_cutoffs, compute_error_metrics\n",
    "working_dir = f'../../forecasting_modules/output/'\n",
    "cutoffs = compute_timeseries_split_cutoffs(\n",
    "    df_hist.index,\n",
    "    horizon=len(df_forecast.index),\n",
    "    delta=len(df_forecast.index),\n",
    "    folds=5,\n",
    "    min_train_size=30*24\n",
    ")\n",
    "smard_res = []\n",
    "results = {}\n",
    "smard_metrics = {}\n",
    "for i, cutoff in enumerate(cutoffs):\n",
    "    mask = (df_hist.index >= cutoff) & (df_hist.index < cutoff + pd.Timedelta(hours=len(df_forecast.index)))\n",
    "    mask_ = (df_smard.index >= cutoff) & (df_smard.index < cutoff + pd.Timedelta(hours=len(df_forecast.index)))\n",
    "    actual = df_hist[target][mask]\n",
    "    predicted = df_smard[f\"{target}_forecasted\"][mask_]\n",
    "    # print(f\"{predicted.index[0]}\")\n",
    "    # print(f\"\\t{predicted.index[-1]}\")\n",
    "    df = pd.DataFrame({\n",
    "        f'{target}_actual':actual.values,\n",
    "        f'{target}_fitted': predicted.values,\n",
    "        f'{target}_lower': np.zeros_like(actual.values),\n",
    "        f'{target}_upper': np.zeros_like(actual.values)\n",
    "    }, index=actual.index)\n",
    "    smard_res.append(copy.deepcopy(df))\n",
    "    smard_metrics[cutoff] = compute_error_metrics(target, df)\n",
    "\n",
    "smard_metrics_ = [smard_metrics[key] for key in smard_metrics.keys()]\n",
    "ave_metrics = {\n",
    "    key: np.mean( [smard_metrics_[i][key] for i in range(len((smard_metrics_)))] ) for key in list(smard_metrics_[0].keys())\n",
    "}\n",
    "smard_metrics_.append(ave_metrics)"
   ],
   "id": "8775cdc021836123",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Results and Evaluation",
   "id": "93b40b4109faab96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_metric_evolutions(file_path: str, metrics: list, smard_metrics:dict or None):\n",
    "    # Load the dataframe\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Filter relevant columns\n",
    "    for metric in metrics:\n",
    "        if metric not in df.columns:\n",
    "            raise ValueError(f\"Metric '{metric}' not found in the dataframe.\")\n",
    "\n",
    "    # Sort the dataframe by horizon and convert horizon to datetime\n",
    "    df['horizon'] = pd.to_datetime(df['horizon'])\n",
    "    df = df.sort_values(by='horizon')\n",
    "\n",
    "    markers = ['s', 'o', 'v', '^', 'P']\n",
    "\n",
    "    # Plot the metric evolution\n",
    "    fig, axes = plt.subplots(figsize=(8, 2*len(metrics)+1), ncols=1, nrows=len(metrics), sharex=True, sharey=False)\n",
    "    if not hasattr(axes, '__len__'): axes = [axes]\n",
    "    for i, (metric, ax) in enumerate(zip(metrics, axes)):\n",
    "        for marker, model_label in zip(markers, df['model_label'].unique()):\n",
    "            if model_label.__contains__('ensemble'): markerfacecolor = 'black'\n",
    "            else: markerfacecolor = 'None'\n",
    "            method_df_trained = df[(df['model_label'] == model_label) & (df['method'] == 'trained')]\n",
    "            ax.plot(method_df_trained['horizon'], method_df_trained[metric], linestyle='None',\n",
    "                    marker=marker, label=model_label, color='black',\n",
    "                    markerfacecolor=markerfacecolor, markeredgecolor='black', markersize=8\n",
    "                    )\n",
    "            # method_df_forecast = df[(df['model_label'] == model_label) & (df['method'] == 'trained')]\n",
    "            # ax.plot(method_df_forecast['horizon'], method_df_forecast[metric], linestyle='None', marker=marker)\n",
    "        if smard_metrics is not None:\n",
    "            print(smard_metrics.keys())\n",
    "            ax.plot(\n",
    "                pd.to_datetime(list(smard_metrics.keys())),\n",
    "                [smard_metric[metric] for smard_metric in smard_metrics.values()],\n",
    "                linestyle='None',\n",
    "                marker='_',\n",
    "                label='SMARD (day-ahead)',\n",
    "                color='black',\n",
    "                markerfacecolor='black',\n",
    "                markeredgecolor='black',\n",
    "                markersize=8\n",
    "            )\n",
    "        # Set x-ticks at unique horizon values\n",
    "        unique_horizons = df['horizon'].sort_values().unique()\n",
    "        ax.set_xticks(unique_horizons)\n",
    "        ax.set_xticklabels([\n",
    "            h.strftime('%Y-%m-%d') for h in unique_horizons], rotation=0, ha='center'#'right'\n",
    "        )\n",
    "\n",
    "        ax.grid(True, linestyle='-', alpha=0.4)\n",
    "        ax.tick_params(axis='x', direction='in', bottom=True)\n",
    "        ax.tick_params(axis='y', which='both', direction='in', left=True, right=True)\n",
    "        # Set border lines transparent by setting the edge color and alpha\n",
    "        ax.spines['top'].set_edgecolor((1, 1, 1, 0))  # Transparent top border\n",
    "        ax.spines['right'].set_edgecolor((1, 1, 1, 0))  # Transparent right border\n",
    "        ax.spines['left'].set_edgecolor((1, 1, 1, 0))  # Transparent left border\n",
    "        ax.spines['bottom'].set_edgecolor((1, 1, 1, 0))  # Transparent bottom border\n",
    "\n",
    "        # Make x and y ticks transparent\n",
    "        ax.tick_params(axis='x', color=(1, 1, 1, 0))  # Transparent x ticks\n",
    "        ax.tick_params(axis='y', color=(1, 1, 1, 0))  # Transparent y ticks\n",
    "\n",
    "        # ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "        # ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format as \"Dec\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"{metric.upper()} for Several Out-of-Sample Forecasts\")\n",
    "        if i == len(metrics)-1:\n",
    "            ax.legend(fancybox=False, frameon=False,loc='upper right')\n",
    "            ax.set_xlabel(\"Starting Date of the Forecasting Horizon\")\n",
    "        ax.set_ylabel(f\"Horizon Averaged {metric.upper()}\")\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path.replace(\".csv\",\".png\"), dpi=600)\n",
    "    plt.show()\n",
    "plot_metric_evolutions(working_dir+target+'/summary_metrics.csv', metrics=['rmse','smape'], smard_metrics=smard_metrics)"
   ],
   "id": "804f3723fdc58ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from forecasting_modules import ForecastingTaskSingleTarget\n",
    "# Load our past and current forecasts\n",
    "forecast_res:dict = ForecastingTaskSingleTarget._load_trained_model(\n",
    "    target=target,\n",
    "    model_label='ensemble[ElasticNet](XGBoost,ElasticNet)',#\"ensemble[ElasticNet](XGBoost,ElasticNet)\",#'ensemble[XGBoost](XGBoost,ElasticNet)',\n",
    "    working_dir=working_dir,\n",
    "    train_forecast='train',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "n = 4 # number of windows to show\n",
    "\n",
    "smard_forecast:pd.DataFrame = copy.deepcopy(smard_res[-1])\n",
    "smard_forecast[:] = 0 # unknown, as SMARD does not provide week-ahead forecast from now\n",
    "tasks = [\n",
    "    {'model':'SMARD','n':n,'name':'TSO day-ahead forecast','lw':1.0,'color':'blue','ci_alpha':0.0,\n",
    "     'results':smard_res[-n:],'metrics':smard_metrics_[-n-1:],'forecast':None,'ahead':'day'}, #\n",
    "    {'model':'XGBoost','n':n,'name':'Our week-ahead forecast','lw':1.0,'color':'red','ci_alpha':0.0,\n",
    "     'results':forecast_res['results'][-n:],'metrics':forecast_res['metrics'][-n-1:],'forecast':None,'ahead':'week'} # forecast_res['forecast']\n",
    "]"
   ],
   "id": "7462bc86fb99e2d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_time_series_with_residuals(tasks: list[dict], target: str = 'total_grid_load', ylabel: str = '',**kwargs):\n",
    "    '''\n",
    "\n",
    "    Plots forecasts split into adjacent windows. Each window can show several forecasts performed with\n",
    "    different methods listed in 'tasks', where each entry is a dictionary with 'results', 'metrics' and 'forecast'.\n",
    "    Last panel shows the latest forecast (as given in 'forecast' in tasks) and last metrics from 'metrics' in tasks.\n",
    "    Bottom panels show residuals between actual target variable and forecasted.\n",
    "\n",
    "    :param tasks: list[dict] where each dict represents a model's forecasting results that consist of\n",
    "    - tasks[0]['results']:list[pd.DataFrame] list of forecasts for past forecasting winows where each dataframe has:\n",
    "    f'{target}_actual', f'{target}_fitted', f'{target}_lower', 'f'{target}_upper'\n",
    "    - tasks[0]['metrics]:list[dict] list of performance metrics for each forecasted window (RMSE,sMAPE...) where the last\n",
    "    element in the list contains the average metrics\n",
    "    tasks[0]['forecast']:pd.Dataframe -- same as dataframes in 'results' but with the current latest forecast\n",
    "    :param target: str target name\n",
    "    :param label: y-label for the top panels\n",
    "    :param kwargs: additional arguments for plotting\n",
    "    :return: None\n",
    "    '''\n",
    "\n",
    "    plot_residuals = False\n",
    "    if 'residuals' in kwargs:\n",
    "        plot_residuals = kwargs['residuals']\n",
    "\n",
    "    legends_per_panel = False\n",
    "    if 'legends_per_panel' in kwargs:\n",
    "        legends_per_panel = kwargs['legends_per_panel']\n",
    "\n",
    "    label_errs = True\n",
    "    if 'label_errs' in kwargs:\n",
    "        label_errs = kwargs['label_errs']\n",
    "\n",
    "    if 'watermark' in kwargs:\n",
    "        watermark_text = kwargs['watermark']\n",
    "    else:\n",
    "        watermark_text = None\n",
    "\n",
    "    if 'itime' in kwargs:\n",
    "        itime = kwargs['itime']\n",
    "    else:\n",
    "        itime = None\n",
    "\n",
    "    # Determine the maximum number of results across tasks\n",
    "    plot_forecast = False\n",
    "    for task in tasks:\n",
    "        if task['forecast']:\n",
    "            plot_forecast = True\n",
    "\n",
    "    max_n_results = max(len(task['results']) for task in tasks)\n",
    "    if plot_forecast: n_cols = max_n_results + 1  # Plus one for 'forecast'\n",
    "    else: n_cols = max_n_results\n",
    "\n",
    "\n",
    "    if not 'drawstyle' in kwargs: drawstyle='default'\n",
    "    else: drawstyle=kwargs['drawstyle']\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows = 2 if plot_residuals else 1, ncols=n_cols,\n",
    "        figsize=kwargs['figsize'] if 'figsize' in kwargs else (n_cols * 5, 8),\n",
    "        gridspec_kw={\n",
    "            'height_ratios': [3, 1] if plot_residuals else [1],\n",
    "            'hspace': 0.02, 'wspace': 0.01\n",
    "        },\n",
    "        sharex='col', sharey='row'\n",
    "    )\n",
    "\n",
    "    # Define column names\n",
    "    actual_col = f'{target}_actual'\n",
    "    fitted_col = f'{target}_fitted'\n",
    "    lower_col = f'{target}_lower'\n",
    "    upper_col = f'{target}_upper'\n",
    "\n",
    "    # For each column\n",
    "\n",
    "    for i in range(n_cols):\n",
    "        ax_top = axes[0, i] if plot_residuals else axes[i]\n",
    "        ax_bottom = axes[1, i] if plot_residuals else None\n",
    "\n",
    "        # Flag to check if 'Actual' data has been plotted\n",
    "        actual_plotted = False\n",
    "\n",
    "        # For each task\n",
    "        for task in tasks:\n",
    "            name = task['name']\n",
    "            color = task['color']\n",
    "            ci_alpha=task['ci_alpha']\n",
    "            lw=task['lw']\n",
    "            # Determine if the task has data for this column\n",
    "            if i < len(task['results']):\n",
    "                df = task['results'][i]\n",
    "                errs = task['metrics'][i]\n",
    "            elif i == max_n_results:\n",
    "                df = task['forecast']\n",
    "                errs = task['metrics'][i]\n",
    "            else:\n",
    "                continue  # Skip plotting for this task in this column\n",
    "\n",
    "            if itime is not None:\n",
    "                itime_:int = itime - i * len(df)\n",
    "                if itime_ > len(df) - 1: itime_ = len(df)-1\n",
    "                if itime_ < 0: itime_ = 0\n",
    "                mask_actual = df.index <= df.index[itime_]\n",
    "\n",
    "                if task['ahead'] == 'day': itime__ = min(itime_+24 if itime_>0 else 0, len(df)-1)\n",
    "                elif task['ahead'] == 'week': itime__ = min(itime_+7*24 if itime_>0 else 0, len(df)-1)\n",
    "                else:raise KeyError(f'Unrecognized forecast model for index offset')\n",
    "                mask_forecast = df.index <= df.index[itime__]\n",
    "            else:\n",
    "                mask_actual = df.index <= df.index[-1]\n",
    "                mask_forecast = df.index <= df.index[-1]\n",
    "\n",
    "            # Plot 'Actual' data once per subplot\n",
    "            if not actual_plotted : #and i != n_cols-1:\n",
    "                ax_top.plot(df[mask_actual].index, df[mask_actual][actual_col],\n",
    "                            label='Actual', color='black', drawstyle=drawstyle, lw=1.5)\n",
    "                actual_plotted = True\n",
    "\n",
    "            # Plot fitted data\n",
    "            if label_errs and (errs is not None and i != n_cols-1):\n",
    "                label = name + ' '  fr\"RMSE={errs['rmse']:.1f}\" + fr\" sMAPE={errs['smape']:.1f}\"\n",
    "            elif label_errs:\n",
    "                label = name + ' ' fr\"$\\langle$RMSE$\\rangle$={errs['rmse']:.1f}\" \\\n",
    "                        + fr\" $\\langle$sMAPE$\\rangle$={errs['smape']:.1f}\"\n",
    "            else:\n",
    "                label = name\n",
    "            ax_top.plot(df[mask_forecast].index, df[mask_forecast][fitted_col],\n",
    "                        label=label, color=color, drawstyle=drawstyle, lw=lw)\n",
    "\n",
    "            # Plot confidence intervals\n",
    "            if ci_alpha > 0.:\n",
    "                ax_top.fill_between(\n",
    "                    df[mask_forecast].index, df[mask_forecast][lower_col], df[mask_forecast][upper_col],\n",
    "                    color=color, alpha=ci_alpha\n",
    "                )\n",
    "\n",
    "            # Plot residuals in the bottom panel\n",
    "            residuals = (df[actual_col] - df[fitted_col]) #/ df[actual_col]\n",
    "            if ax_bottom: ax_bottom.plot(df[mask_actual].index, residuals[mask_actual], label=name, color=color, drawstyle=drawstyle, lw=lw)\n",
    "\n",
    "            # limit plots\n",
    "            ax_top.set_xlim(df.index.min(),df.index.max())\n",
    "            if ax_bottom: ax_bottom.set_xlim(df.index.min(),df.index.max())\n",
    "            if 'ylim0' in kwargs: ax_top.set_ylim(kwargs['ylim0'][0], kwargs['ylim0'][1])\n",
    "            if 'ylim1' in kwargs and ax_bottom: ax_bottom.set_ylim(kwargs['ylim1'][0], kwargs['ylim1'][1])\n",
    "\n",
    "        # print(f\"N={len(df.index)} idx0={df.index[0].isoformat()}\")\n",
    "        # Add a horizontal line at y=0 in residuals plot\n",
    "        if ax_bottom: ax_bottom.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "        # Set titles and labels\n",
    "        if itime and itime_ > 0:\n",
    "            if i < max_n_results:\n",
    "                ax_top.set_title(f'Week {df.index[-1].isocalendar().week} of 2024', fontsize=14, weight='bold')\n",
    "            else:\n",
    "                ax_top.set_title('Current Forecast', fontsize=14, weight='bold')\n",
    "\n",
    "\n",
    "        if ylabel and i == 0:\n",
    "            ax_top.set_ylabel(ylabel)\n",
    "            if ax_bottom: ax_bottom.set_ylabel('Residual / Actual')\n",
    "\n",
    "        # legend in the empty area in residual plots\n",
    "        if i == n_cols - 1 and ax_bottom:\n",
    "            ax_bottom.legend(loc='upper left', ncol=1, fontsize=10)\n",
    "\n",
    "        if legends_per_panel or i==0:#i == n_cols - 1:\n",
    "            ax_top.legend(loc='lower left', ncol=1, fontsize=10)\n",
    "\n",
    "        if watermark_text and i == n_cols-1:\n",
    "            ax_top.text(0.1, 0.1, watermark_text, transform=ax_top.transAxes,\n",
    "                        fontsize=40, color='gray', alpha=0.2,\n",
    "                        ha='center', va='center', rotation=0)\n",
    "\n",
    "        for ax in [ax_top, ax_bottom] if plot_residuals else [ax_top]:\n",
    "            ax.grid(True, linestyle='-', alpha=0.4)\n",
    "            ax.tick_params(axis='x', direction='in', bottom=True)\n",
    "            ax.tick_params(axis='y', which='both', direction='in', left=True, right=True)\n",
    "            # Set border lines transparent by setting the edge color and alpha\n",
    "            ax.spines['top'].set_edgecolor((1, 1, 1, 0))  # Transparent top border\n",
    "            ax.spines['right'].set_edgecolor((1, 1, 1, 0))  # Transparent right border\n",
    "            ax.spines['left'].set_edgecolor((1, 1, 1, 0))  # Transparent left border\n",
    "            ax.spines['bottom'].set_edgecolor((1, 1, 1, 0))  # Transparent bottom border\n",
    "\n",
    "            # Make x and y ticks transparent\n",
    "            ax.tick_params(axis='x', color=(1, 1, 1, 0))  # Transparent x ticks\n",
    "            ax.tick_params(axis='y', color=(1, 1, 1, 0))  # Transparent y ticks\n",
    "\n",
    "            # Make x and y tick labels transparent\n",
    "            # for tick_label in ax.get_xticklabels():\n",
    "            #     tick_label.set_color((1, 1, 1, 0))  # Transparent x tick labels\n",
    "            # for tick_label in ax.get_yticklabels():\n",
    "            #     tick_label.set_color((1, 1, 1, 0))  # Transparent y tick labels\n",
    "\n",
    "\n",
    "        # Improve x-axis formatting\n",
    "        # ax_bottom.set_xlabel(f'Date (month-day for $2024$)', fontsize=12)\n",
    "        if ax_bottom:\n",
    "            ax_bottom.xaxis.set_major_locator(mdates.DayLocator())\n",
    "            # ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "            ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format as \"Dec 15\"\n",
    "        else:\n",
    "            ax_top.xaxis.set_major_locator(mdates.DayLocator())\n",
    "            # ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "            ax_top.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format as \"Dec 15\"\n",
    "        fig.autofmt_xdate(rotation=45)\n",
    "\n",
    "\n",
    "    model_names = \"\".join(task[\"name\"]+'_' for task in tasks)\n",
    "    if 'fpath' in kwargs:\n",
    "        plt.savefig(kwargs['fpath'], bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{target}_{model_names}.png', bbox_inches='tight')\n",
    "    if 'show' in kwargs and kwargs['show']: plt.show()\n",
    "\n",
    "    plt.close(fig)\n",
    "    gc.collect()\n",
    "\n",
    "# Usage: Just call this function similarly to your existing plotting call\n",
    "plot_time_series_with_residuals(\n",
    "    tasks, target=target, ylabel='Offshore Wind Power',\n",
    "    residuals = False, legends_per_panel=False, label_errs=False, figsize=(12,4),\n",
    "    show=True,\n",
    "    itime = 900, fpath = \"./results.png\" #fpath=os.getcwd()+'/'+'movie'+'/'+f\"{i:03}.png\",\n",
    ")"
   ],
   "id": "43bf684adb6b26d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "836b7a069d8233aa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
