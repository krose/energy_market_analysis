{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import copy\n",
    "from itertools import cycle\n",
    "from typing import Optional, Union\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from glob import glob\n",
    "import requests_cache\n",
    "import numpy as np\n",
    "from sympy.physics.units import years\n",
    "\n",
    "from data_collection_modules import DataEnergySMARD\n",
    "from data_collection_modules import locations, OpenMeteo"
   ],
   "id": "277503e23704893c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load All Dataframes",
   "id": "9925f0c4c3b6130a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "data_dir = '../database/'\n",
    "target = 'wind_offshore'\n",
    "# df = pd.read_parquet(data_dir + 'latest.parquet')\n",
    "df_smard = pd.read_parquet(data_dir + 'smard/' + 'history.parquet')\n",
    "df_om = pd.read_parquet(data_dir + 'openmeteo/' + 'history.parquet')\n",
    "df_om_f = pd.read_parquet(data_dir + 'openmeteo/' + 'forecast.parquet')\n",
    "df_es = pd.read_parquet(data_dir + 'epexspot/' + 'history.parquet')\n",
    "\n",
    "print(f\"SMARD data shapes hist={df_smard.shape} start={df_smard.index[0]} end={df_smard.index[-1]}\")\n",
    "print(f\"Openmeteo data shapes hist={df_om.shape} start={df_om.index[0]} end={df_om.index[-1]}\")\n",
    "print(f\"Openmeteo data shapes forecast={df_om_f.shape} start={df_om_f.index[0]} end={df_om_f.index[-1]}\")\n",
    "print(f\"EPEXSPOT data shapes hist={df_es.shape} start={df_es.index[0]} end={df_es.index[-1]}\")\n",
    "\n",
    "print(f\"Target={target} Nans={df_smard[target].isna().sum().sum()}\")\n",
    "# set how to split the dataset\n",
    "cutoff = df_om_f.index[0]\n",
    "if cutoff == cutoff.normalize():\n",
    "    print(f\"The cutoff timestamp corresponds to the beginning of the day {cutoff.normalize()}\")\n",
    "print(f\"Dataset is split into ({len(df_om[:cutoff])}) before and \"\n",
    "      f\"({len(df_om_f[cutoff:])}) ({int(len(df_om_f[cutoff:])/24)} days) after {cutoff}.\")\n",
    "print(df_smard.columns.to_list())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# df_smard = pd.read_parquet(data_dir + 'smard/' + 'history.parquet')\n",
    "# df_smard_ = df_smard[:df_smard.index[-1]-timedelta(days=180)]\n",
    "# df_smard_.to_parquet(data_dir + 'smard/' + 'history.parquet')"
   ],
   "id": "a209f4ed67afabb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Engineering For Weather Data (Historic and Forecast)",
   "id": "fc80c858812d488b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create combined dataframe\n",
    "cutoff = df_om_f.index[0]\n",
    "df_om = df_om.combine_first(df_om_f)\n",
    "if df_om.isna().any().any():\n",
    "    print(\"ERROR! Nans in the dataframe\")\n",
    "def preprocess_openmeteo_for_offshore_wind_OLD(df, location_suffix=\"_hsee\"):\n",
    "    \"\"\"\n",
    "    Preprocesses weather data for forecasting offshore wind energy generation.\n",
    "    \"\"\"\n",
    "    # 1. Filter for relevant location\n",
    "    cols_to_keep = [c for c in df.columns if c.endswith(location_suffix)]\n",
    "    df = df[cols_to_keep].copy()\n",
    "\n",
    "    # 2. Key variable columns\n",
    "    wind_speed_10m_col = f\"wind_speed_10m{location_suffix}\"\n",
    "    wind_speed_100m_col = f\"wind_speed_100m{location_suffix}\"\n",
    "    wind_dir_10m_col = f\"wind_direction_10m{location_suffix}\"\n",
    "    wind_dir_100m_col = f\"wind_direction_100m{location_suffix}\"\n",
    "    temp_col = f\"temperature_2m{location_suffix}\"\n",
    "    press_col = f\"surface_pressure{location_suffix}\"\n",
    "    rh_col = f\"relative_humidity_2m{location_suffix}\"\n",
    "\n",
    "    # 3. Wind direction (cyclic encoding)\n",
    "    if wind_dir_10m_col in df.columns:\n",
    "        df[\"wind_dir_10m_sin\"] = np.sin(np.deg2rad(df[wind_dir_10m_col]))\n",
    "        df[\"wind_dir_10m_cos\"] = np.cos(np.deg2rad(df[wind_dir_10m_col]))\n",
    "        df.drop(columns=[wind_dir_10m_col], inplace=True, errors=\"ignore\")\n",
    "    if wind_dir_100m_col in df.columns:\n",
    "        df[\"wind_dir_100m_sin\"] = np.sin(np.deg2rad(df[wind_dir_100m_col]))\n",
    "        df[\"wind_dir_100m_cos\"] = np.cos(np.deg2rad(df[wind_dir_100m_col]))\n",
    "        df.drop(columns=[wind_dir_100m_col], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # 4. Wind speed transformations\n",
    "    if wind_speed_10m_col in df.columns and wind_speed_100m_col in df.columns:\n",
    "        # Wind Shear\n",
    "        df[\"wind_shear\"] = np.log(df[wind_speed_100m_col] / df[wind_speed_10m_col]) / np.log(100 / 10)\n",
    "        # Wind energy potential (v^3 for both heights)\n",
    "        df[\"wind_power_potential_10m\"] = df[wind_speed_10m_col] ** 3\n",
    "        df[\"wind_power_potential_100m\"] = df[wind_speed_100m_col] ** 3\n",
    "\n",
    "        # Lag and rolling features for wind speed at 100m\n",
    "        for lag in [1, 6, 12, 24]:\n",
    "            df[f\"{wind_speed_100m_col}_lag{lag}\"] = df[wind_speed_100m_col].shift(lag)\n",
    "        df[f\"{wind_speed_100m_col}_roll6\"] = df[wind_speed_100m_col].rolling(window=6, min_periods=1).mean()\n",
    "        df[f\"{wind_speed_100m_col}_roll24\"] = df[wind_speed_100m_col].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "    # 5. Turbulence Intensity\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        rolling_std = df[wind_speed_100m_col].rolling(window=3, min_periods=1).std()\n",
    "        rolling_mean = df[wind_speed_100m_col].rolling(window=3, min_periods=1).mean()\n",
    "        df[\"turbulence_intensity\"] = rolling_std / rolling_mean\n",
    "\n",
    "    # 6. Wind Ramp Events\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        df[\"wind_ramp\"] = df[wind_speed_100m_col].diff(periods=1)  # Change in wind speed over 1 timestep\n",
    "\n",
    "    # 7. Moisture Index\n",
    "    if temp_col in df.columns and rh_col in df.columns:\n",
    "        df[\"moisture_index\"] = df[temp_col] * df[rh_col]\n",
    "\n",
    "    # 8. Thermal Stability Index\n",
    "    if temp_col in df.columns and press_col in df.columns:\n",
    "        df[\"thermal_stability_index\"] = (df[temp_col] + 273.15) * (1000 / df[press_col]) ** 0.286\n",
    "\n",
    "    # 9. Drop irrelevant features\n",
    "    drop_vars = [\"precipitation\", \"cloud_cover\", \"shortwave_radiation\"]\n",
    "    drop_cols = [f\"{var}{location_suffix}\" for var in drop_vars if f\"{var}{location_suffix}\" in df.columns]\n",
    "    df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return df\n",
    "def preprocess_openmeteo_for_offshore_wind(df, location_suffix=\"_hsee\")->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses weather data for forecasting offshore wind energy generation.\n",
    "    Focuses on critical physical features and includes turbulence_intensity, wind_ramp, and wind_shear.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Filter for the offshore wind farm location only\n",
    "    cols_to_keep = [c for c in df.columns if c.endswith(location_suffix)]\n",
    "    df = df[cols_to_keep].copy()\n",
    "\n",
    "    # 2. Define key variable columns\n",
    "    wind_speed_10m_col = f\"wind_speed_10m{location_suffix}\"\n",
    "    wind_speed_100m_col = f\"wind_speed_100m{location_suffix}\"\n",
    "    wind_dir_100m_col = f\"wind_direction_100m{location_suffix}\"\n",
    "    temp_col = f\"temperature_2m{location_suffix}\"\n",
    "    press_col = f\"surface_pressure{location_suffix}\"\n",
    "\n",
    "    # 3. Compute Air Density (ρ)\n",
    "    if temp_col in df.columns and press_col in df.columns:\n",
    "        temp_K = df[temp_col] + 273.15\n",
    "        R_specific = 287.05  # J/(kg·K) for dry air\n",
    "        # convert pressure from hPa to Pa\n",
    "        df[\"air_density\"] = np.array( (df[press_col] * 100.) / (R_specific * temp_K) )\n",
    "\n",
    "    # 4. Compute Wind Power Density (if wind_speed_100m and air_density are available)\n",
    "    if wind_speed_100m_col in df.columns and \"air_density\" in df.columns:\n",
    "        df[\"wind_power_density\"] = np.array( 0.5 * df[\"air_density\"] * (df[wind_speed_100m_col] ** 3) )\n",
    "\n",
    "    # 5. Encode Wind Direction (Cyclic)\n",
    "    if wind_dir_100m_col in df.columns:\n",
    "        df[\"wind_dir_sin\"] = np.sin(np.deg2rad(df[wind_dir_100m_col]))\n",
    "        df[\"wind_dir_cos\"] = np.cos(np.deg2rad(df[wind_dir_100m_col]))\n",
    "        df.drop(columns=[wind_dir_100m_col], inplace=True)\n",
    "\n",
    "    # 6. Wind Shear (Requires both 10m and 100m wind speeds)\n",
    "    if wind_speed_10m_col in df.columns and wind_speed_100m_col in df.columns:\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df[\"wind_shear\"] = np.log(df[wind_speed_100m_col] / df[wind_speed_10m_col]) / np.log(100/10)\n",
    "        # Replace infinities or NaNs if they occur\n",
    "        df[\"wind_shear\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 7. Turbulence Intensity (using a short rolling window on 100m wind speed)\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        rolling_std = df[wind_speed_100m_col].rolling(window=3, min_periods=1).std()\n",
    "        rolling_mean = df[wind_speed_100m_col].rolling(window=3, min_periods=1).mean()\n",
    "        df[\"turbulence_intensity\"] = np.array( rolling_std / rolling_mean )\n",
    "\n",
    "    # 8. Wind Ramp (difference in 100m wind speed over 1 timestep)\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        df[\"wind_ramp\"] = df[wind_speed_100m_col].diff(1)\n",
    "\n",
    "    # 9. Lag Features for Wind Speed at 100m\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        for lag in [1, 6, 12, 24]:\n",
    "            df[f\"wind_speed_lag_{lag}\"] = df[wind_speed_100m_col].shift(lag)\n",
    "\n",
    "    # 11. Drop Irrelevant Columns\n",
    "    # Decide which columns to drop. For model simplicity, consider dropping raw weather inputs\n",
    "    # that have been transformed into more physical parameters.\n",
    "    # However, keep wind speeds if you think they add value.\n",
    "    # For now, we keep the wind speeds since other derived features depend on them.\n",
    "    drop_vars = [\n",
    "        temp_col, press_col, \"air_density\",\n",
    "        f\"precipitation{location_suffix}\",\n",
    "        f\"cloud_cover{location_suffix}\",\n",
    "        f\"shortwave_radiation{location_suffix}\",\n",
    "        f\"relative_humidity_2m{location_suffix}\",\n",
    "        f\"wind_direction_10m{location_suffix}\",\n",
    "        f\"wind_gusts_10m{location_suffix}\"\n",
    "    ]\n",
    "    drop_cols = [c for c in drop_vars if c in df.columns]\n",
    "    df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Handle missing values introduced by lagging and other computations\n",
    "    # df.dropna(inplace=True)\n",
    "\n",
    "    return df\n",
    "df_om_prep = preprocess_openmeteo_for_offshore_wind(df=df_om, location_suffix=\"_hsee\")\n",
    "print(df_om_prep.shape, df_om_prep.columns, df_om_prep.isna().sum())\n",
    "df_om_prep.dropna(inplace=True)"
   ],
   "id": "ca3d11c66e2f67dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_weather_smard(df_om_prep_, df_smard_, target=target):\n",
    "    # df_om_prep_ = df_om_prep[:cutoff].tail(30*24)\n",
    "    # df_smard_ = df_smard.tail(30*24)\n",
    "    fig,axes = plt.subplots(ncols=1,nrows=6,figsize=(10,10), sharex='all')\n",
    "    ax = axes[0]\n",
    "    ax.plot(df_smard_.index, df_smard_[target])\n",
    "    ax = axes[1]\n",
    "    ax.plot(df_om_prep_.index, df_om_prep_[\"wind_speed_100m_hsee\"])\n",
    "    ax = axes[2]\n",
    "    ax.plot(df_om_prep_.index, df_om_prep_[\"wind_shear\"])\n",
    "    ax = axes[3]\n",
    "    ax.plot(df_om_prep_.index, df_om_prep_[\"turbulence_intensity\"])\n",
    "    ax = axes[4]\n",
    "    ax.plot(df_om_prep_.index, df_om_prep_[\"wind_ramp\"])\n",
    "    ax = axes[5]\n",
    "    ax.plot(df_om_prep_.index, df_om_prep_[\"wind_power_density\"])\n",
    "    plt.show()\n",
    "visualize_weather_smard(df_om_prep[:cutoff].tail(90*24), df_smard.tail(90*24))"
   ],
   "id": "e879dd3ea2110d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Split Data Into Hist and Forecast and Combine With SMARD Data",
   "id": "c3c6fc1eb70eeecb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize the split between data\n",
    "if cutoff == cutoff.normalize():\n",
    "    print(\"The cutoff timestamp corresponds to the beginning of the day.\")\n",
    "print(cutoff)\n",
    "df_om_prep[:cutoff-timedelta(hours=1)].tail(30*24)['wind_speed_10m_hsee'].plot()\n",
    "df_om_prep[cutoff:].head(24)['wind_speed_10m_hsee'].plot()\n",
    "plt.axvline(cutoff)"
   ],
   "id": "39bd9685d1d21729",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def handle_nans_with_interpolation(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks each column of the DataFrame for NaNs. If a column has more than 3 consecutive NaNs,\n",
    "    it raises a ValueError. Otherwise, fills the NaNs using bi-directional interpolation.\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    def check_consecutive_nans(series: pd.Series):\n",
    "        # Identify consecutive NaNs by grouping non-NaN segments and counting consecutive NaNs\n",
    "        consecutive_nans = (series.isna().astype(int)\n",
    "                            .groupby((~series.isna()).cumsum())\n",
    "                            .cumsum())\n",
    "        if consecutive_nans.max() > 3:\n",
    "            raise ValueError(f\"Column '{series.name}' in {name} contains more than 3 consecutive NaNs.\")\n",
    "\n",
    "    # Check all columns for consecutive NaNs first\n",
    "    for col in df_copy.columns:\n",
    "        check_consecutive_nans(df_copy[col])\n",
    "\n",
    "    # Interpolate all columns at once after confirming they're valid\n",
    "    df_copy = df_copy.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "\n",
    "    return df_copy\n",
    "def fix_broken_periodicity_with_interpolation(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fixes broken hourly periodicity by adding missing timestamps if fewer than 3 consecutive are missing.\n",
    "    Raises an error if more than 3 consecutive timestamps are missing.\n",
    "    Missing values are filled using time-based interpolation.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(f\"The DataFrame {name} must have a datetime index.\")\n",
    "\n",
    "    expected_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "    missing_timestamps = expected_index.difference(df.index)\n",
    "\n",
    "    if missing_timestamps.empty:\n",
    "        print(f\"The DataFrame {name} is already hourly with no missing segments.\")\n",
    "        return df\n",
    "\n",
    "    # Convert to a Series to check consecutive missing timestamps\n",
    "    missing_series = pd.Series(missing_timestamps)\n",
    "    groups = (missing_series.diff() != pd.Timedelta(hours=1)).cumsum()\n",
    "\n",
    "    # Check if any group has more than 3 missing points\n",
    "    group_counts = groups.value_counts()\n",
    "    if (group_counts > 3).any():\n",
    "        bad_group = group_counts[group_counts > 3].index[0]\n",
    "        raise ValueError(f\"More than 3 consecutive missing timestamps detected: \"\n",
    "                         f\"{missing_series[groups == bad_group].values} in {name}\")\n",
    "\n",
    "    # Reindex and interpolate\n",
    "    fixed_df = df.reindex(expected_index)\n",
    "    fixed_df = fixed_df.interpolate(method='time')\n",
    "\n",
    "    print(f\"Added and interpolated {len(missing_timestamps)} missing timestamps in {name}.\")\n",
    "\n",
    "    return fixed_df\n",
    "def validate_dataframe(df: pd.DataFrame, name: str = '') -> pd.DataFrame:\n",
    "    \"\"\"Check for NaNs, missing values, and periodicity in a time-series DataFrame.\"\"\"\n",
    "\n",
    "    # Check for NaNs\n",
    "    if df.isnull().any().any():\n",
    "        print(f\"ERROR! {name} DataFrame contains NaN values.\")\n",
    "        df = handle_nans_with_interpolation(df, name)\n",
    "\n",
    "    # Check if index is sorted in ascending order\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        print(f\"ERROR! {name} The index is not in ascending order.\")\n",
    "        raise ValueError(\"Data is not in ascending order.\")\n",
    "\n",
    "    # Check for hourly frequency with no missing segments\n",
    "    full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "    if not full_range.equals(df.index):\n",
    "        print(f\"ERROR! {name} The data is not hourly or has missing segments.\")\n",
    "        df = fix_broken_periodicity_with_interpolation(df, name)\n",
    "\n",
    "    return df\n",
    "\n",
    "horizon = 7 * 24\n",
    "# merger with SMRD target column\n",
    "target = 'wind_offshore'\n",
    "df_om_prep.dropna(inplace=True, how='any')\n",
    "df_hist = pd.merge(\n",
    "    df_om_prep[:cutoff-timedelta(hours=1)],\n",
    "    df_smard[:cutoff-timedelta(hours=1)][target],\n",
    "    left_index=True, right_index=True, how=\"inner\"\n",
    ")\n",
    "df_forecast = df_om_prep[cutoff : cutoff+timedelta(hours=horizon - 1)]\n",
    "df_hist = validate_dataframe(df_hist, 'df_hist')\n",
    "df_forecast = validate_dataframe(df_forecast, 'df_forecast')\n",
    "df_hist = df_hist[df_hist.index[-1]-pd.Timedelta(hours = 100 * horizon - 1):]\n",
    "print(f\"Features {len(df_hist.columns)-1} hist.shape={df_hist.shape} ({int(len(df_hist)/horizon)}) forecast.shape={df_forecast.shape}\")\n",
    "print(f\"Hist: from {df_hist.index[0]} to {df_hist.index[-1]} ({len(df_hist)/horizon})\")\n",
    "print(f\"Fore: from {df_forecast.index[0]} to {df_forecast.index[-1]} ({len(df_forecast)/horizon})\")\n",
    "\n",
    "outdir = \"../tmp_database/\"\n",
    "df_hist.to_parquet(f\"{outdir}history.parquet\")\n",
    "df_forecast.to_parquet(f\"{outdir}forecast.parquet\")"
   ],
   "id": "eaf4819d14851387",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_hist[target].tail(512).plot()",
   "id": "96a7f2f7aaabcea9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5a55b2c010e0267c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize",
   "id": "d6e4a5b6227ac86e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4075b868a5de2007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from forecasting_modules import compute_timeseries_split_cutoffs, compute_error_metrics\n",
    "\n",
    "cutoffs = compute_timeseries_split_cutoffs(\n",
    "    df_hist.index,\n",
    "    horizon=len(df_forecast.index),\n",
    "    delta=len(df_forecast.index),\n",
    "    folds=5,\n",
    "    min_train_size=30*24\n",
    ")\n",
    "smard_res = []\n",
    "results = {}\n",
    "smard_metrics = []\n",
    "for i, cutoff in enumerate(cutoffs):\n",
    "    mask = (df_hist.index >= cutoff) & (df_hist.index < cutoff + pd.Timedelta(hours=len(df_forecast.index)))\n",
    "    mask_ = (df_smard.index >= cutoff) & (df_smard.index < cutoff + pd.Timedelta(hours=len(df_forecast.index)))\n",
    "    actual = df_hist[target][mask]\n",
    "    predicted = df_smard[f\"{target}_forecasted\"][mask_]\n",
    "    # print(f\"{predicted.index[0]}\")\n",
    "    # print(f\"\\t{predicted.index[-1]}\")\n",
    "    df = pd.DataFrame({\n",
    "        f'{target}_actual':actual.values,\n",
    "        f'{target}_fitted': predicted.values,\n",
    "        f'{target}_lower': np.zeros_like(actual.values),\n",
    "        f'{target}_upper': np.zeros_like(actual.values)\n",
    "    }, index=actual.index)\n",
    "    smard_res.append(copy.deepcopy(df))\n",
    "    smard_metrics.append( compute_error_metrics(target, df) )\n",
    "\n",
    "\n",
    "ave_metrics = {\n",
    "    key: np.mean( [smard_metrics[i][key] for i in range(len((smard_metrics)))] ) for key in list(smard_metrics[0].keys())\n",
    "}\n",
    "smard_metrics.append(ave_metrics)\n"
   ],
   "id": "4f937ae0cfe5bf56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "working_dir = f'../forecasting_modules/output/'\n",
    "from forecasting_modules import ForecastingTaskSingleTarget\n",
    "# Load our past and current forecasts\n",
    "forecast_res:dict = ForecastingTaskSingleTarget._load_trained_model(\n",
    "    target=target,\n",
    "    model_label=\"ensemble[XGBoost](XGBoost,ElasticNet)\",#'ensemble[XGBoost](XGBoost,ElasticNet)',\n",
    "    working_dir=working_dir,\n",
    "    train_forecast='forecast',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "n = 4 # number of windows to show\n",
    "\n",
    "\n",
    "smard_forecast:pd.DataFrame = copy.deepcopy(smard_res[-1])\n",
    "smard_forecast[:] = 0 # unknown, as SMARD does not provide week-ahead forecast from now\n",
    "tasks = [\n",
    "    {'model':'SMARD','n':n,'name':'TSO Day-ahead forecast','lw':1.0,'color':'blue','ci_alpha':0.0,\n",
    "     'results':smard_res[-n:],'metrics':smard_metrics[-n-1:],'forecast':None}, #\n",
    "    {'model':'XGBoost','n':n,'name':'Our Week-ahead Forecast','lw':1.0,'color':'red','ci_alpha':0.0,\n",
    "     'results':forecast_res['results'][-n:],'metrics':forecast_res['metrics'][-n-1:],'forecast':None} # forecast_res['forecast']\n",
    "]\n",
    "# plot_time_series_with_residuals(\n",
    "#     tasks=tasks,\n",
    "#     target=target,\n",
    "#     ylabel='Off-shore Wind Generation',\n",
    "# )fig,a\n",
    "fig,ax = plt.subplots()\n",
    "smard_res[-1][f'{target}_actual'].plot(ax=ax, color='blue')\n",
    "forecast_res['results'][-1][f'{target}_actual'].plot(ax=ax, color='red')"
   ],
   "id": "68634d57eecad44e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_time_series_with_residuals(tasks: list[dict], target: str = 'total_grid_load', ylabel: str = '',**kwargs):\n",
    "    '''\n",
    "\n",
    "    Plots forecasts split into adjacent windows. Each window can show several forecasts performed with\n",
    "    different methods listed in 'tasks', where each entry is a dictionary with 'results', 'metrics' and 'forecast'.\n",
    "    Last panel shows the latest forecast (as given in 'forecast' in tasks) and last metrics from 'metrics' in tasks.\n",
    "    Bottom panels show residuals between actual target variable and forecasted.\n",
    "\n",
    "    :param tasks: list[dict] where each dict represents a model's forecasting results that consist of\n",
    "    - tasks[0]['results']:list[pd.DataFrame] list of forecasts for past forecasting winows where each dataframe has:\n",
    "    f'{target}_actual', f'{target}_fitted', f'{target}_lower', 'f'{target}_upper'\n",
    "    - tasks[0]['metrics]:list[dict] list of performance metrics for each forecasted window (RMSE,sMAPE...) where the last\n",
    "    element in the list contains the average metrics\n",
    "    tasks[0]['forecast']:pd.Dataframe -- same as dataframes in 'results' but with the current latest forecast\n",
    "    :param target: str target name\n",
    "    :param label: y-label for the top panels\n",
    "    :param kwargs: additional arguments for plotting\n",
    "    :return: None\n",
    "    '''\n",
    "\n",
    "    plot_residuals = False\n",
    "    if 'residuals' in kwargs:\n",
    "        plot_residuals = kwargs['residuals']\n",
    "\n",
    "    legends_per_panel = False\n",
    "    if 'legends_per_panel' in kwargs:\n",
    "        legends_per_panel = kwargs['legends_per_panel']\n",
    "\n",
    "    label_errs = True\n",
    "    if 'label_errs' in kwargs:\n",
    "        label_errs = kwargs['label_errs']\n",
    "\n",
    "    # Determine the maximum number of results across tasks\n",
    "    plot_forecast = False\n",
    "    for task in tasks:\n",
    "        if task['forecast']:\n",
    "            plot_forecast = True\n",
    "\n",
    "    max_n_results = max(len(task['results']) for task in tasks)\n",
    "    if plot_forecast: n_cols = max_n_results + 1  # Plus one for 'forecast'\n",
    "    else: n_cols = max_n_results\n",
    "\n",
    "\n",
    "    if not 'drawstyle' in kwargs: drawstyle='default'\n",
    "    else: drawstyle=kwargs['drawstyle']\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows = 2 if plot_residuals else 1, ncols=n_cols,\n",
    "        figsize=kwargs['figsize'] if 'figsize' in kwargs else (n_cols * 5, 8),\n",
    "        gridspec_kw={\n",
    "            'height_ratios': [3, 1] if plot_residuals else [1],\n",
    "            'hspace': 0.02, 'wspace': 0.01\n",
    "        },\n",
    "        sharex='col', sharey='row'\n",
    "    )\n",
    "\n",
    "    # Define column names\n",
    "    actual_col = f'{target}_actual'\n",
    "    fitted_col = f'{target}_fitted'\n",
    "    lower_col = f'{target}_lower'\n",
    "    upper_col = f'{target}_upper'\n",
    "\n",
    "    # For each column\n",
    "\n",
    "    for i in range(n_cols):\n",
    "        ax_top = axes[0, i] if plot_residuals else axes[i]\n",
    "        ax_bottom = axes[1, i] if plot_residuals else None\n",
    "\n",
    "        # Flag to check if 'Actual' data has been plotted\n",
    "        actual_plotted = False\n",
    "\n",
    "        # For each task\n",
    "        for task in tasks:\n",
    "            name = task['name']\n",
    "            color = task['color']\n",
    "            ci_alpha=task['ci_alpha']\n",
    "            lw=task['lw']\n",
    "            # Determine if the task has data for this column\n",
    "            if i < len(task['results']):\n",
    "                df = task['results'][i]\n",
    "                errs = task['metrics'][i]\n",
    "            elif i == max_n_results:\n",
    "                df = task['forecast']\n",
    "                errs = task['metrics'][i]\n",
    "            else:\n",
    "                continue  # Skip plotting for this task in this column\n",
    "\n",
    "            # Plot 'Actual' data once per subplot\n",
    "            if not actual_plotted : #and i != n_cols-1:\n",
    "                ax_top.plot(df.index, df[actual_col], label='Actual', color='black', drawstyle=drawstyle, lw=1.5)\n",
    "                actual_plotted = True\n",
    "\n",
    "            # Plot fitted data\n",
    "            if label_errs and (errs is not None and i != n_cols-1):\n",
    "                label = name + ' '  fr\"RMSE={errs['rmse']:.1f}\" + fr\" sMAPE={errs['smape']:.1f}\"\n",
    "            elif label_errs:\n",
    "                label = name + ' ' fr\"$\\langle$RMSE$\\rangle$={errs['rmse']:.1f}\" \\\n",
    "                         + fr\" $\\langle$sMAPE$\\rangle$={errs['smape']:.1f}\"\n",
    "            else:\n",
    "                label = name\n",
    "            ax_top.plot(df.index, df[fitted_col], label=label, color=color, drawstyle=drawstyle, lw=lw)\n",
    "\n",
    "            # Plot confidence intervals\n",
    "            if ci_alpha > 0.:\n",
    "                ax_top.fill_between(df.index, df[lower_col], df[upper_col], color=color, alpha=ci_alpha)\n",
    "\n",
    "            # Plot residuals in the bottom panel\n",
    "            residuals = (df[actual_col] - df[fitted_col]) #/ df[actual_col]\n",
    "            if ax_bottom: ax_bottom.plot(df.index, residuals, label=name, color=color, drawstyle=drawstyle, lw=lw)\n",
    "\n",
    "            # limit plots\n",
    "            ax_top.set_xlim(df.index.min(),df.index.max())\n",
    "            if ax_bottom: ax_bottom.set_xlim(df.index.min(),df.index.max())\n",
    "            if 'ylim0' in kwargs: ax_top.set_ylim(kwargs['ylim0'][0], kwargs['ylim0'][1])\n",
    "            if 'ylim1' in kwargs and ax_bottom: ax_bottom.set_ylim(kwargs['ylim1'][0], kwargs['ylim1'][1])\n",
    "\n",
    "        # print(f\"N={len(df.index)} idx0={df.index[0].isoformat()}\")\n",
    "        # Add a horizontal line at y=0 in residuals plot\n",
    "        if ax_bottom: ax_bottom.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "        # Set titles and labels\n",
    "        if i < max_n_results:\n",
    "            ax_top.set_title(f'Week {df.index[-1].isocalendar().week} of 2024', fontsize=14, weight='bold')\n",
    "        else:\n",
    "            ax_top.set_title('Current Forecast', fontsize=14, weight='bold')\n",
    "\n",
    "        if ylabel and i == 0:\n",
    "            ax_top.set_ylabel(ylabel)\n",
    "            if ax_bottom: ax_bottom.set_ylabel('Residual / Actual')\n",
    "\n",
    "        # legend in the empty area in residual plots\n",
    "        if i == n_cols - 1 and ax_bottom:\n",
    "            ax_bottom.legend(loc='upper left', ncol=1, fontsize=10)\n",
    "\n",
    "        if legends_per_panel or i == n_cols - 1:\n",
    "            ax_top.legend(loc='upper left', ncol=1, fontsize=10)\n",
    "\n",
    "        for ax in [ax_top, ax_bottom] if plot_residuals else [ax_top]:\n",
    "            ax.grid(True, linestyle='-', alpha=0.4)\n",
    "            ax.tick_params(axis='x', direction='in', bottom=True)\n",
    "            ax.tick_params(axis='y', which='both', direction='in', left=True, right=True)\n",
    "            # Set border lines transparent by setting the edge color and alpha\n",
    "            ax.spines['top'].set_edgecolor((1, 1, 1, 0))  # Transparent top border\n",
    "            ax.spines['right'].set_edgecolor((1, 1, 1, 0))  # Transparent right border\n",
    "            ax.spines['left'].set_edgecolor((1, 1, 1, 0))  # Transparent left border\n",
    "            ax.spines['bottom'].set_edgecolor((1, 1, 1, 0))  # Transparent bottom border\n",
    "\n",
    "            # Make x and y ticks transparent\n",
    "            ax.tick_params(axis='x', color=(1, 1, 1, 0))  # Transparent x ticks\n",
    "            ax.tick_params(axis='y', color=(1, 1, 1, 0))  # Transparent y ticks\n",
    "\n",
    "            # Make x and y tick labels transparent\n",
    "            # for tick_label in ax.get_xticklabels():\n",
    "            #     tick_label.set_color((1, 1, 1, 0))  # Transparent x tick labels\n",
    "            # for tick_label in ax.get_yticklabels():\n",
    "            #     tick_label.set_color((1, 1, 1, 0))  # Transparent y tick labels\n",
    "\n",
    "\n",
    "        # Improve x-axis formatting\n",
    "        # ax_bottom.set_xlabel(f'Date (month-day for $2024$)', fontsize=12)\n",
    "        if ax_bottom:\n",
    "            ax_bottom.xaxis.set_major_locator(mdates.DayLocator())\n",
    "            # ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "            ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format as \"Dec 15\"\n",
    "        else:\n",
    "            ax_top.xaxis.set_major_locator(mdates.DayLocator())\n",
    "            # ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "            ax_top.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format as \"Dec 15\"\n",
    "        fig.autofmt_xdate(rotation=45)\n",
    "\n",
    "\n",
    "    model_names = \"\".join(task[\"name\"]+'_' for task in tasks)\n",
    "    plt.savefig(f'{target}_{model_names}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Usage: Just call this function similarly to your existing plotting call\n",
    "plot_time_series_with_residuals(tasks, target=target, ylabel='Off-shore Wind Generation', residuals = False, legends_per_panel=False, label_errs=False, figsize=(12,4))"
   ],
   "id": "e7edbc620824ed18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_broken_periodicity(df, target):\n",
    "    \"\"\"\n",
    "    Plots the DataFrame and highlights where periodicity is broken.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): A pandas DataFrame with a datetime index.\n",
    "    \"\"\"\n",
    "    # Ensure the index is datetime\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"The DataFrame must have a datetime index.\")\n",
    "\n",
    "    # Generate the expected hourly index\n",
    "    expected_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "    # Find missing or unexpected timestamps\n",
    "    missing_timestamps = expected_index.difference(df.index)\n",
    "    extra_timestamps = df.index.difference(expected_index)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[target], marker='.', label='Data')\n",
    "\n",
    "    # Highlight missing timestamps\n",
    "    for ts in missing_timestamps:\n",
    "        plt.axvline(ts, color='red', linestyle='--', alpha=0.7, label='Missing Period' if ts == missing_timestamps[0] else \"\")\n",
    "\n",
    "    # Highlight unexpected timestamps\n",
    "    for ts in extra_timestamps:\n",
    "        plt.axvline(ts, color='orange', linestyle='--', alpha=0.7, label='Unexpected Period' if ts == extra_timestamps[0] else \"\")\n",
    "\n",
    "    # Add legend and labels\n",
    "    plt.title(\"Data Plot with Highlighted Broken Periodicity\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def handle_nans_with_interpolation(df: pd.DataFrame, name:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks each column of the DataFrame for NaNs. If a column has more than 3 consecutive NaNs,\n",
    "    it raises a ValueError. Otherwise, it fills the NaNs using bi-directional interpolation.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with time series data (indexed by pd.Timestamp).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with NaNs filled using bi-directional interpolation.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any column contains more than 3 consecutive NaNs.\n",
    "    \"\"\"\n",
    "    def check_consecutive_nans(series: pd.Series):\n",
    "        # Find consecutive NaNs\n",
    "        consecutive_nans = series.isna().astype(int).groupby(series.notna().cumsum()).cumsum()\n",
    "        # Check if any sequence of NaNs exceeds 3\n",
    "        if consecutive_nans.max() > 3:\n",
    "            raise ValueError(f\"Column '{series.name}' in {name} contains more than 3 consecutive NaNs.\")\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    for column in df_copy.columns:\n",
    "        # Check if the column has more than 3 consecutive NaNs\n",
    "        check_consecutive_nans(df_copy[column])\n",
    "        # Fill NaNs using bi-directional interpolation\n",
    "        df_copy[column] = df_copy[column].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    return df_copy\n",
    "def fix_broken_periodicity_with_interpolation(df:pd.DataFrame, name:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fixes broken hourly periodicity by adding missing timestamps if fewer than 3 consecutive points are missing.\n",
    "    Raises an error if more than 3 consecutive timestamps are missing.\n",
    "    Missing values are filled using interpolation.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): A pandas DataFrame with a datetime index.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with missing points added and values interpolated where necessary.\n",
    "    \"\"\"\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(f\"The DataFrame {name} must have a datetime index.\")\n",
    "\n",
    "    # Generate the full expected hourly range\n",
    "    expected_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "\n",
    "    # Find missing timestamps\n",
    "    missing_timestamps = expected_index.difference(df.index)\n",
    "\n",
    "    if missing_timestamps.empty:\n",
    "        print(f\"The DataFrame {name} is already hourly with no missing segments.\")\n",
    "        return df\n",
    "\n",
    "    # Identify consecutive missing groups\n",
    "    missing_diffs = missing_timestamps.to_series().diff().dt.seconds\n",
    "    consecutive_missing_groups = missing_diffs != 3600  # Check if time diff isn't exactly 1 hour\n",
    "\n",
    "    # Label consecutive groups\n",
    "    missing_timestamps_df = pd.DataFrame({'timestamp': missing_timestamps})\n",
    "    missing_timestamps_df['group'] = consecutive_missing_groups.cumsum()\n",
    "\n",
    "    # Check for groups with more than 3 missing points\n",
    "    for group_id, group in missing_timestamps_df.groupby('group'):\n",
    "        if len(group) > 3:\n",
    "            raise ValueError(f\"More than 3 consecutive missing timestamps detected: {group['timestamp'].values} in {name}\")\n",
    "\n",
    "    # Add missing timestamps back to the DataFrame\n",
    "    fixed_df = df.reindex(df.index.union(missing_timestamps).sort_values())\n",
    "\n",
    "    # Interpolate to fill missing values\n",
    "    fixed_df = fixed_df.interpolate(method='time')\n",
    "\n",
    "    print(f\"Added and interpolated {len(missing_timestamps)} missing timestamps in {name}.\")\n",
    "\n",
    "    return fixed_df\n",
    "def validate_dataframe(df, name:str=''):\n",
    "    ''' check for nans, missing values and preiodicity in a time-series dataframe '''\n",
    "    # Check for NaNs\n",
    "    has_nans = df.isnull().any().any()\n",
    "    if has_nans:\n",
    "        print(f\"ERROR! {name} DataFrame contains NaN values.\")\n",
    "        df = handle_nans_with_interpolation(df, name)\n",
    "\n",
    "    # Check if the index is in ascending order\n",
    "    index_ascending = df.index.is_monotonic_increasing\n",
    "    if not index_ascending:\n",
    "        print(f\"ERROR! {name} The index is not in ascending order.\")\n",
    "        raise ValueError(\"Data is not in ascending order.\")\n",
    "\n",
    "    # Check if the index is hourly with no missing segments\n",
    "    is_hourly = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h').equals(df.index)\n",
    "    if not is_hourly:\n",
    "        print(f\"ERROR! {name} The data is not hourly or has missing segments.\")\n",
    "        df = fix_broken_periodicity_with_interpolation(df, name)\n",
    "    return df\n",
    "\n",
    "# merger with SMRD target column\n",
    "target = 'wind_offshore'\n",
    "df_om_prep.dropna(inplace=True, how='any')\n",
    "df_hist = pd.merge(df_om_prep, df_smard[target], left_index=True, right_index=True, how=\"inner\")\n",
    "df_forecast:pd.DataFrame = df_om_prep[cutoff+timedelta(hours=1) : cutoff+timedelta(hours=1 + 7*24)]\n",
    "df_hist:pd.DataFrame = validate_dataframe(df_hist, 'df_hist')\n",
    "df_forecast = validate_dataframe(df_forecast, 'df_forecast')\n",
    "df_hist = df_hist[df_hist.index[-1]-pd.Timedelta(weeks=50):]\n",
    "print(f\"Features {len(df_hist.columns)-1} hist.shape={df_hist.shape} ({int(len(df_hist) / 7 / 24)} weeks) forecast.shape={df_forecast.shape}\")\n"
   ],
   "id": "9794c45e6e7a89fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from forecasting_modules import compute_timeseries_split_cutoffs, compute_error_metrics\n",
    "cutoffs = compute_timeseries_split_cutoffs(\n",
    "    df_hist.index,\n",
    "    horizon=len(df_forecast.index),\n",
    "    delta=len(df_forecast.index),\n",
    "    folds=5,\n",
    "    min_train_size=30*24\n",
    ")\n",
    "dfs = []\n",
    "results = {}\n",
    "metrics = []\n",
    "for i, cutoff in enumerate(cutoffs):\n",
    "    print(f\"Processing {i}/{len(cutoffs)}\")\n",
    "    mask = (df_hist.index > cutoff) & (df_hist.index <= cutoff + pd.Timedelta(hours=len(df_forecast.index)))\n",
    "    mask_ = (df_smard.index > cutoff) & (df_smard.index <= cutoff + pd.Timedelta(hours=len(df_forecast.index)))\n",
    "    actual = df_hist[target][mask]\n",
    "    predicted = df_smard[f\"{target}_forecasted\"][mask_]\n",
    "    df = pd.DataFrame({\n",
    "        f'{target}_actual':actual.values,\n",
    "        f'{target}_fitted': predicted.values,\n",
    "        f'{target}_lower': np.zeros_like(actual.values),\n",
    "        f'{target}_upper': np.zeros_like(actual.values)\n",
    "    }, index=actual.index)\n",
    "    dfs.append(copy.deepcopy(df))\n",
    "    metrics.append( compute_error_metrics(target, df) )\n",
    "\n",
    "ave_metrics = {\n",
    "    key: np.mean( [metrics[i][key] for i in range(len((metrics)))] ) for key in list(metrics[0].keys())\n",
    "}\n",
    "metrics.append(ave_metrics)\n",
    "print(len(metrics))"
   ],
   "id": "8bd2e04671d7d3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_modules import plot_time_series_with_residuals\n",
    "n = 3\n",
    "forecast = copy.deepcopy(dfs[-1])\n",
    "forecast[:] = 0\n",
    "tasks = [\n",
    "    {'model':'SMARD','n':n,'name':'SMARD','lw':0.7,'color':'red','ci_alpha':0.0,\n",
    "     'results':dfs[-n:],'metrics':metrics[-n-1:],'forecast':forecast}\n",
    "]\n",
    "plot_time_series_with_residuals(\n",
    "    tasks=tasks,\n",
    "    target=target,\n",
    "    ylabel='Off-shore Wind Generation'\n",
    ")"
   ],
   "id": "7c3d62dff18453eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preprocess SMARD data\n",
    "def aggregate_smard_data(df_smard, countries:list):\n",
    "    # 1. Creating the 'other_gen' column\n",
    "    df_smard['other_gen'] = df_smard[['biomass', 'hydropower', 'lignite', 'hard_coal', 'natural_gas',\n",
    "                                      'pumped_storage', 'other_conventional', 'other_renewables']].sum(axis=1)\n",
    "\n",
    "    # 2. Creating 'net_import' and 'net_export' columns\n",
    "    import_columns = [f\"{country}_import\" for country in countries]\n",
    "    export_columns = [f\"{country}_export\" for country in countries]\n",
    "\n",
    "    df_smard['net_import'] = df_smard[import_columns].sum(axis=1)\n",
    "    df_smard['net_export'] = df_smard[export_columns].sum(axis=1)\n",
    "\n",
    "    # 3. Selecting the required columns for the new dataframe\n",
    "    selected_columns = ['other_gen', 'net_import', 'net_export', 'total_grid_load',\n",
    "                        'residual_load', 'solar', 'wind_offshore', 'wind_onshore']\n",
    "\n",
    "    df_new = df_smard[selected_columns]\n",
    "df_smard_ = aggregate_smard_data(df_smard, list(DataEnergySMARD.country_map.keys()))"
   ],
   "id": "cd56f24ff936e38f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_data_for_offshore_wind_gen(df_om, df_om_f):\n",
    "    df = df\n",
    "    location_suffix=\"_hsee\"\n",
    "    # 1. Filter for relevant location\n",
    "    cols_to_keep = [c for c in df.columns if c.endswith(location_suffix)]\n",
    "    df = df[cols_to_keep].copy()"
   ],
   "id": "93b053a6c517f0fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_df_hist_df_forecast(verbose:bool):\n",
    "    data_dir = '../database/'\n",
    "    # df = pd.read_parquet(data_dir + 'latest.parquet')\n",
    "    df_smard = pd.read_parquet(data_dir + 'smard/' + 'history.parquet')\n",
    "\n",
    "    def aggregate_smard_data(df_smard, countries:list):\n",
    "        # 1. Creating the 'other_gen' column\n",
    "        df_smard['other_gen'] = df_smard[['biomass', 'hydropower', 'lignite', 'hard_coal', 'natural_gas',\n",
    "                                          'pumped_storage', 'other_conventional', 'other_renewables']].sum(axis=1)\n",
    "\n",
    "        # 2. Creating 'net_import' and 'net_export' columns\n",
    "        import_columns = [f\"{country}_import\" for country in countries]\n",
    "        export_columns = [f\"{country}_export\" for country in countries]\n",
    "\n",
    "        df_smard['net_import'] = df_smard[import_columns].sum(axis=1)\n",
    "        df_smard['net_export'] = df_smard[export_columns].sum(axis=1)\n",
    "\n",
    "        # 3. Selecting the required columns for the new dataframe\n",
    "        selected_columns = ['other_gen', 'net_import', 'net_export', 'total_grid_load',\n",
    "                            'residual_load', 'solar', 'wind_offshore', 'wind_onshore']\n",
    "\n",
    "        df_new = df_smard[selected_columns]\n",
    "    df_smard_ = aggregate_smard_data(df_smard, list(DataEnergySMARD.country_map.keys()))\n",
    "\n",
    "    def aggregate_openmeteo_data(df_smard, locations_:list):\n",
    "\n",
    "\n",
    "\n",
    "    df_om = pd.read_parquet(data_dir + 'openmeteo/' + 'history.parquet')\n",
    "    df_om_f = pd.read_parquet(data_dir + 'openmeteo/' + 'forecast.parquet')\n",
    "    df_es = pd.read_parquet(data_dir + 'epexspot/' + 'history.parquet')\n",
    "\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"SMARD data shapes hist={df_smard.shape} start={df_smard.index[0]} end={df_smard.index[-1]}\")\n",
    "        print(f\"Openmeteo data shapes hist={df_om.shape} start={df_om.index[0]} end={df_om.index[-1]}\")\n",
    "        print(f\"Openmeteo data shapes forecast={df_om_f.shape} start={df_om_f.index[0]} end={df_om_f.index[-1]}\")\n",
    "        print(f\"EPEXSPOT data shapes hist={df_es.shape} start={df_es.index[0]} end={df_es.index[-1]}\")\n",
    "\n",
    "    df_hist = pd.merge(df_smard, df_om, left_index=True, right_index=True)\n",
    "    df_hist = pd.merge(df_hist, df_es, left_index=True, right_index=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Merged hist dataframes shape\")\n",
    "\n"
   ],
   "id": "7672e975fb194709",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_smard.tail(n=5)",
   "id": "f80b8c0f816cadba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_gen_load_with_forecasts(df_smard_):\n",
    "    \n",
    "    df_smard_ = df_smard_[~(df_smard_[['total_grid_load','residual_load','solar','wind_offshore','wind_onshore']] == 0).all(axis=1)]\n",
    "    \n",
    "    fig,axes = plt.subplots(figsize=(10,10), nrows=6, sharex='all')\n",
    "    ax = axes[0]\n",
    "    \n",
    "    ax.plot(df_smard_.index,df_smard_['total_grid_load'], color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['total_grid_load_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Total Load')\n",
    "    \n",
    "    ax = axes[1]\n",
    "    ax.plot(df_smard_.index,df_smard_['residual_load'], color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['residual_load_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Residual Load')\n",
    "    \n",
    "    ax = axes[2]\n",
    "    ax.plot(df_smard_.index,df_smard_['solar'], color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['solar_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Solar Generation')\n",
    "    \n",
    "    ax = axes[3]\n",
    "    ax.plot(df_smard_.index,df_smard_['wind_offshore'], color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['wind_offshore_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Wind Off-shore')\n",
    "    \n",
    "    ax = axes[4]\n",
    "    ax.plot(df_smard_.index,df_smard_['wind_onshore'], color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['wind_onshore_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Wind On-shore')\n",
    "    \n",
    "    ax = axes[5]\n",
    "    ax.plot(df_smard_.index,df_smard_[['biomass','hydropower','lignite','hard_coal','natural_gas','pumped_storage','other_conventional','other_renewables']].aggregate(func='sum',axis=1), color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['other_gen_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Other Generation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_gen_load_with_forecasts(df_smard.tail(30*24))"
   ],
   "id": "a1d17b8e7f48dea6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_collection_modules import DataEnergySMARD\n",
    "def plot_cross_border_trade_smard(df_smard_, by_country:bool):\n",
    "    if by_country:\n",
    "        fig,axes = plt.subplots(figsize=(10,12), nrows=len(list(DataEnergySMARD.country_map.keys()))+1, sharex='all')\n",
    "    else: \n",
    "        fig,ax = plt.subplots(figsize=(10,6), nrows=1)\n",
    "        axes = [ax]\n",
    "    if by_country:\n",
    "        for ax, country in zip(axes, list(DataEnergySMARD.country_map.keys())):\n",
    "            import_ = df_smard_[f\"{country}_import\"]\n",
    "            export_ = df_smard_[f\"{country}_export\"]\n",
    "            # ax.plot(df_smard_.index,import_, color='blue', label='Import')\n",
    "            # ax.plot(df_smard_.index,export_, color='red', label='Export')\n",
    "            ax.plot(df_smard_.index,import_ + export_, color='black', label='Trade')\n",
    "            ax.set_ylabel(country)\n",
    "            ax.axhline(0,color='gray',linestyle=':')\n",
    "\n",
    "    net_import = df_smard_[[f\"{country}_import\" for country in list(DataEnergySMARD.country_map.keys())]].aggregate(func='sum',axis=1)\n",
    "    net_export = df_smard_[[f\"{country}_export\" for country in list(DataEnergySMARD.country_map.keys())]].aggregate(func='sum',axis=1)\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    window_size = 24  # Adjust the window size as needed\n",
    "    net_import_ma = net_import.rolling(window=window_size).mean()\n",
    "    net_export_ma = net_export.rolling(window=window_size).mean()\n",
    "    net_sum_ma = (net_import + net_export).rolling(window=window_size).mean()\n",
    "    \n",
    "    # Plot the original data\n",
    "    axes[-1].plot(df_smard_.index, net_import, color='blue', label='Import')\n",
    "    axes[-1].plot(df_smard_.index, net_export, color='red', label='Export')\n",
    "    axes[-1].plot(df_smard_.index, net_export + net_import, color='black', label='Sum')\n",
    "    \n",
    "    # Plot the moving averages\n",
    "    axes[-1].plot(df_smard_.index, net_import_ma, color='blue', linestyle='--', label='Import (MA)')\n",
    "    axes[-1].plot(df_smard_.index, net_export_ma, color='red', linestyle='--', label='Export (MA)')\n",
    "    axes[-1].plot(df_smard_.index, net_sum_ma, color='black', linestyle='--', label='Sum (MA)')\n",
    "    \n",
    "    # Add reference line and legend\n",
    "    axes[-1].axhline(0, color='gray', linestyle=':')\n",
    "    axes[-1].legend(loc='upper right')\n",
    "    \n",
    "    # Adjust layout and show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_cross_border_trade_smard(df_smard.tail(n=30*24), by_country=False)"
   ],
   "id": "8841b5cb9252553c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OPENMETEO",
   "id": "7a8a30b23deb2f94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_collection_modules import locations, OpenMeteo\n",
    "def plot_openmeteo(df, df_f, suffix):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=len(list(OpenMeteo.variables_standard)), sharex='all', figsize=(10,12))\n",
    "    for ax, quantity in zip(axes, list(OpenMeteo.variables_standard)):\n",
    "        ax.plot(df.index, df[f\"{quantity}{suffix}\"], color='black', ls='-', label='Historic')\n",
    "        ax.plot(df_f.index, df_f[f\"{quantity}{suffix}\"], color='black', ls='--', label='Forecast')\n",
    "        ax.set_ylabel(quantity)\n",
    "\n",
    "        ave = df[[f\"{quantity}{loc['suffix']}\" for loc in locations]].aggregate(func='mean',axis=1)\n",
    "        ave_f = df[[f\"{quantity}{loc['suffix']}\" for loc in locations]].aggregate(func='mean',axis=1)\n",
    "\n",
    "        ax.plot(ave.index, ave, color='gray', ls='-', label='Historic')\n",
    "        ax.plot(ave_f.index, ave_f, color='gray', ls='--', label='Forecast')\n",
    "        ax.set_ylabel(quantity)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_openmeteo(df=df_om.tail(n=30*24),df_f=df_om_f,suffix='_hsee')\n",
    "    "
   ],
   "id": "1a60634e8e59d0a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_energy_data(df):\n",
    "    # Ensure datetime index\n",
    "    df_month = df.tail(30*24)\n",
    "\n",
    "    # List of countries\n",
    "    countries = ['austria', 'belgium', 'czechia', 'denmark', 'france', 'luxembourg',\n",
    "                 'netherlands', 'norway', 'poland', 'sweden', 'switzerland']\n",
    "\n",
    "    # Sum '_export' and '_import' columns for each country\n",
    "    df_flows = pd.DataFrame()\n",
    "    for country in countries:\n",
    "        df_flows[country] = df_month[f'{country}_export'] + df_month[f'{country}_import']\n",
    "\n",
    "    # Remaining columns\n",
    "    remaining_columns = ['other_gen', 'residual_load_forecast', 'solar', 'total_gen',\n",
    "                         'total_grid_load', 'wind_offshore', 'wind_onshore']\n",
    "    df_remaining = df_month[remaining_columns]\n",
    "\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 10))\n",
    "\n",
    "    # First panel: stacked bar chart of df_flows\n",
    "    df_flows.plot(kind='bar', stacked=True, ax=axes[0], width=1.0, linewidth=0)\n",
    "\n",
    "    axes[0].set_title('Hourly Energy Flows by Country')\n",
    "    axes[0].set_ylabel('Total Energy Flow')\n",
    "    axes[0].legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "    axes[0].set_xlabel('')\n",
    "\n",
    "    # Adjust x-axis ticks on the bar chart\n",
    "    num_ticks = len(df_flows)\n",
    "    tick_positions = range(0, num_ticks, 24)  # Show a tick every 24 bars (once per day)\n",
    "    tick_labels = df_flows.index[::24].strftime('%Y-%m-%d')\n",
    "    axes[0].set_xticks(tick_positions)\n",
    "    axes[0].set_xticklabels(tick_labels, rotation=45, ha='right')\n",
    "\n",
    "    # Second panel: lines of remaining columns\n",
    "    df_remaining.plot(ax=axes[1])\n",
    "    axes[1].set_title('Hourly Other Energy Data')\n",
    "    axes[1].set_ylabel('Value')\n",
    "    axes[1].legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "    # Adjust x-axis ticks on the second panel\n",
    "    axes[1].xaxis.set_major_locator(mdates.DayLocator(interval=3))\n",
    "    axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    axes[1].set_xlabel('Date')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "visualize_energy_data(df_smard)"
   ],
   "id": "cc013b71793c0a8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "E",
   "id": "aa72c6caf175cb8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Combined Weather and Energy",
   "id": "410f57a09b173ff9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_smard_openmeteo(df_smard, df_om, df_om_f, om_suffix):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 10),sharex='all')\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax_ = ax.twinx()\n",
    "    ax.plot(df_smard.index, df_smard['solar'], color='black', label='Solar Energy Generation')\n",
    "    ax.set_ylabel('Solar Energy Generation')\n",
    "    ax_.plot(df_om.index, df_om[f'shortwave_radiation{om_suffix}'], color='red', label='Historic')\n",
    "    ax_.plot(df_om_f.index, df_om_f[f'shortwave_radiation{om_suffix}'], color='red', ls='--', label='Forecast')\n",
    "    ax_.set_ylabel('Shortwave Radiation')\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax_ = ax.twinx()\n",
    "    ax.plot(df_smard.index, df_smard['wind_onshore'], color='black', label='Solar Energy Generation')\n",
    "    ax.set_ylabel('On-shore Wind Generation')\n",
    "    ax_.plot(df_om.index, df_om[f'wind_speed_10m{om_suffix}'], color='red', label='Historic')\n",
    "    ax_.plot(df_om_f.index, df_om_f[f'wind_speed_10m{om_suffix}'], color='red', ls='--', label='Forecast')\n",
    "    ax.axvline(df_om_f.index[0])\n",
    "    ax_.set_ylabel('Wind Speed')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_smard_openmeteo(df_smard.tail(30*24), df_om.tail(30*24), df_om_f, om_suffix='_hsee')\n",
    "    "
   ],
   "id": "65029a895b6849cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_es.tail(24*30).plot(figsize=(16,5))",
   "id": "e2eaa2f9de299bc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7caa79a74550c91e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_es.tail()",
   "id": "3827f061486e62c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_time_series(df, df_f, suffix, figsize=(15, 20)):\n",
    "    \"\"\"\n",
    "    Visualizes each column with a specified suffix in separate panels with a shared X-axis.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame with time-series data indexed by pd.Timestamp.\n",
    "    - suffix (str): Suffix to filter the columns of interest.\n",
    "    - figsize (tuple): Size of the figure for the plots.\n",
    "    \"\"\"\n",
    "    # Filter columns based on the suffix\n",
    "    filtered_columns = [col for col in df.columns if col.endswith(suffix)]\n",
    "\n",
    "    if not filtered_columns:\n",
    "        raise ValueError(f\"No columns found with suffix '{suffix}' in the DataFrame.\")\n",
    "\n",
    "    num_columns = len(filtered_columns)\n",
    "\n",
    "    # Create a shared X-axis for all subplots\n",
    "    fig, axes = plt.subplots(num_columns, 1, figsize=figsize, sharex=True)\n",
    "    if num_columns == 1:\n",
    "        axes = [axes]  # Ensure axes is always iterable\n",
    "\n",
    "    for ax, column in zip(axes, filtered_columns):\n",
    "        ax.plot(df.index, df[column], label=column)\n",
    "        ax.plot(df_f.index, df_f[column], ls='--')\n",
    "        ax.set_ylabel(column.replace(suffix, '').replace('_', ' ').capitalize())\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Set shared X-axis label\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "visualize_time_series(df=df_om,df_f = df_om_f, suffix='hsee')"
   ],
   "id": "4096d2079bfd61c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "def",
   "id": "b676f6b909e6689c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "def plot_multivariate_timeseries(df, df_fore):\n",
    "    # Create a subplot with 3 rows\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.02)\n",
    "\n",
    "    # Plot 'DA_auction_price' on the first subplot\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['DA_auction_price'], name='DA Auction Price'),\n",
    "                  row=1, col=1)\n",
    "\n",
    "    # Plot 'total_gen', 'total_grid_load', 'residual_load_forecast' on the second subplot\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['total_gen'], name='Total Generation'), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['total_grid_load'], name='Total Grid Load'), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['residual_load_forecast'], name='Residual Load Forecast'), row=2, col=1)\n",
    "\n",
    "    # Plot 'temperature_2m_hsee' and 'temperature_2m_solw' on the third subplot\n",
    "    for df_,ls in zip([df, df_fore],['solid', 'dash']):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_.index,\n",
    "            y=df_['temperature_2m_hsee'],\n",
    "            name='Temperature 2m HSEE',\n",
    "            line=dict(dash=ls,color='blue')\n",
    "        ), row=3, col=1)\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_.index,\n",
    "            y=df_['temperature_2m_solw'],\n",
    "            name='Temperature 2m SOLW',\n",
    "            line=dict(dash=ls,color='green')\n",
    "        ), row=3, col=1)\n",
    "    # Add a vertical line for today across all subplots\n",
    "    # today_line = dict(type='line', x0=pd.Timestamp.today(), y0=0, x1=pd.Timestamp.today(),\n",
    "    #                   y1=1, xref='x', yref='paper', line=dict(color='black', width=2, dash='dash'))\n",
    "    # fig.add_shape(today_line, row='all', col=1)\n",
    "    fig.add_vline(x=pd.Timestamp.today(), line_width=1, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(height=900, title='Multivariate Time Series Analysis',\n",
    "                      xaxis_title='DateTime', yaxis_title='Values')\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "plot_multivariate_timeseries(df=df_hist.tail(1000), df_fore = df_fore)"
   ],
   "id": "57ad1223135ac35b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot SMARD data",
   "id": "11c82d1e0362cc38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_modules.collect_data_smard import DataEnergySMARD\n",
    "countries = list(DataEnergySMARD.country_map.keys())\n",
    "# Create a subplot with 3 rows\n",
    "fig = make_subplots(rows=len(countries), cols=1, shared_xaxes=True, vertical_spacing=0.02)\n",
    "\n",
    "df_hist_ = df_hist.tail(4*7*24)\n",
    "for i, country in enumerate(countries):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_hist_.index,\n",
    "        y=df_hist_[f'{country}_export'],\n",
    "        name=f'{country}',\n",
    "        line=dict(color='blue')  # Set the line color here\n",
    "    ), row=i+1, col=1)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_hist_.index,\n",
    "        y=df_hist_[f'{country}_import'],\n",
    "        name=None,#f'{country}',\n",
    "        line=dict(color='red')  # Set the line color here\n",
    "    ), row=i+1, col=1)\n",
    "# Update layout\n",
    "fig.update_layout(height=1800, title='International trade',\n",
    "xaxis_title='DateTime', yaxis_title='Values')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ],
   "id": "c3c3dcbfafabbfd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6e5cac98f5e74214",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
